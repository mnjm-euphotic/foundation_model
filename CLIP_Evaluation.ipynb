{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f5b555-f3d1-47a1-83c4-ddabc2628bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from glob import glob\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbcf3093-b5bc-47df-a3ff-e2aaba4e7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import clip\n",
    "# import torch\n",
    "# from torchvision.datasets import CIFAR100\n",
    "\n",
    "# # Load the model\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# # Download the dataset\n",
    "# cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
    "\n",
    "# # Prepare the inputs\n",
    "# image, class_id = cifar100[3637]\n",
    "# image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "# text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
    "\n",
    "# # Calculate features\n",
    "# with torch.no_grad():\n",
    "#     image_features = model.encode_image(image_input)\n",
    "#     text_features = model.encode_text(text_inputs)\n",
    "\n",
    "# # Pick the top 5 most similar labels for the image\n",
    "# image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "# text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "# similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "# values, indices = similarity[0].topk(5)\n",
    "\n",
    "# # Print the result\n",
    "# print(\"\\nTop predictions:\\n\")\n",
    "# for value, index in zip(values, indices):\n",
    "#     print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56000358-4ea7-41f4-8ad0-343811231c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "avail_models = [ 'RN50', 'RN50x64', 'ViT-B/32', 'ViT-L/14@336px' ]\n",
    "\n",
    "model, preprocess = clip.load(avail_models[2], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb72c691-d650-4716-8ee2-c42639080eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7207\n"
     ]
    }
   ],
   "source": [
    "dataset = \"/home/mnjm/workspace/clip/dataset/onion/*/*/*.jpg\"\n",
    "imgs_l = glob(dataset)\n",
    "imgs_l = [ x for x in imgs_l if \"cropped\" not in x ]\n",
    "print(len(imgs_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af1859ce-47cb-49a4-965b-98f3ceede9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0834,  0.0474,  0.0749,  ...,  0.0463, -0.0332,  0.0344],\n",
      "        [-0.0727,  0.0376,  0.0495,  ...,  0.0397, -0.0219,  0.0050],\n",
      "        [-0.0446,  0.0228,  0.0587,  ...,  0.0628, -0.0233,  0.0036]])\n"
     ]
    }
   ],
   "source": [
    "labels = ['raw', 'translucent', 'golden brown']\n",
    "text_inputs = torch.cat([clip.tokenize(f\"{label} chopped onions in a dark pan\") for label in labels]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embedds = model.encode_text(text_inputs)\n",
    "text_embedds /= text_embedds.norm(dim=-1, keepdim=True)\n",
    "print(text_embedds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bbcf46a-1a19-43fc-8a7d-4452454f5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, c = 0, 0\n",
    "true_lbl_l, pred_lbl_l = [], []\n",
    "for img_p in imgs_l:\n",
    "    img = Image.open(img_p).convert(\"RGB\")\n",
    "    img_cv = cv2.imread(img_p)\n",
    "    img_cv = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
    "    if img_cv.shape[0] == 128 and img_cv.shape[1] == 128: continue\n",
    "    t += 1\n",
    "    img = preprocess(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        img_embedd = model.encode_image(img)\n",
    "    img_embedd /= img_embedd.norm(dim=-1, keepdim=True)\n",
    "    similarity = (img_embedd @ text_embedds.T).softmax(dim=-1)\n",
    "    similarity = np.array(similarity).reshape(-1)\n",
    "    pred_lbl = np.argmax(similarity)\n",
    "    score = np.max(similarity) * 100.0\n",
    "    true_lbl = int(img_p.split(os.path.sep)[-2])\n",
    "    if true_lbl != pred_lbl:\n",
    "        c += 1\n",
    "        # plt.title(f\"{labels[pred_lbl]}({score:.2f}) (True: {labels[true_lbl]})\")\n",
    "        # plt.imshow(img_cv)\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40da7b4a-9ed0-4847-a952-184eaec19e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3782/7131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 0), dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(f\"{c}/{t}\")\n",
    "assert len(true_lbl_l) == len(pred_lbl_l)\n",
    "confusion_matrix(true_lbl_l, pred_lbl_l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
