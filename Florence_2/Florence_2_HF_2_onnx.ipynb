{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ee38918-b425-4b90-b386-879c06b7eed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 14:49:38.045268: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-21 14:49:38.211720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732180778.299744   54638 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732180778.328239   54638 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-21 14:49:38.492957: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM, AutoModel, AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from optimum.exporters import TasksManager\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd5621d-238a-48e0-b207-7e8840ef5ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnjm/.py_venv/clip/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model_id = \"microsoft/Florence-2-base\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, trust_remote_code=True, torch_dtype=\"float32\"\n",
    ").eval()\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b61306f-36c8-4c6b-8712-95b84758f5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Florence2ForConditionalGeneration(\n",
       "  (vision_tower): DaViT(\n",
       "    (convs): ModuleList(\n",
       "      (0): ConvEmbed(\n",
       "        (proj): Conv2d(3, 128, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ConvEmbed(\n",
       "        (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ConvEmbed(\n",
       "        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ConvEmbed(\n",
       "        (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): MySequential(\n",
       "        (0): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.004)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.004)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): MySequential(\n",
       "        (0): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.009)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.009)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.013)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.013)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): MySequential(\n",
       "        (0): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.017)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.017)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.022)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.022)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.026)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.026)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.030)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.030)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.035)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.035)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.039)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.039)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.043)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.043)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.048)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.048)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.052)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.052)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.057)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.057)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.061)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.061)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.065)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.065)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.070)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.070)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.074)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.074)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.078)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.078)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.083)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.083)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.087)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.087)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.091)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.091)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): MySequential(\n",
       "        (0): MySequential(\n",
       "          (spatial_block): SpatialBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (window_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.096)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.096)\n",
       "            )\n",
       "          )\n",
       "          (channel_block): ChannelBlock(\n",
       "            (conv1): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (channel_attn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): ChannelAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.100)\n",
       "            )\n",
       "            (conv2): PreNorm(\n",
       "              (fn): DepthWiseConv2d(\n",
       "                (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "            )\n",
       "            (ffn): PreNorm(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Mlp(\n",
       "                (net): Sequential(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): DropPath(drop_prob=0.100)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (image_proj_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (image_pos_embed): LearnedAbsolutePositionEmbedding2D(\n",
       "    (row_embeddings): Embedding(50, 512)\n",
       "    (column_embeddings): Embedding(50, 512)\n",
       "  )\n",
       "  (visual_temporal_embed): PositionalEmbeddingCosine1D()\n",
       "  (language_model): Florence2LanguageForConditionalGeneration(\n",
       "    (model): Florence2LanguageModel(\n",
       "      (shared): Embedding(51289, 768, padding_idx=1)\n",
       "      (encoder): Florence2Encoder(\n",
       "        (embed_tokens): Florence2ScaledWordEmbedding(51289, 768, padding_idx=1)\n",
       "        (embed_positions): Florence2LearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x Florence2EncoderLayer(\n",
       "            (self_attn): Florence2SdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Florence2Decoder(\n",
       "        (embed_tokens): Florence2ScaledWordEmbedding(51289, 768, padding_idx=1)\n",
       "        (embed_positions): Florence2LearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x Florence2DecoderLayer(\n",
       "            (self_attn): Florence2SdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): Florence2SdpaAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=51289, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c37b9d2b-95ca-44ec-8c4b-3093df0edb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Florence2Processor:\n",
       "- image_processor: CLIPImageProcessor {\n",
       "  \"auto_map\": {\n",
       "    \"AutoProcessor\": \"microsoft/Florence-2-base--processing_florence2.Florence2Processor\"\n",
       "  },\n",
       "  \"crop_size\": {\n",
       "    \"height\": 768,\n",
       "    \"width\": 768\n",
       "  },\n",
       "  \"do_center_crop\": false,\n",
       "  \"do_convert_rgb\": null,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
       "  \"image_seq_length\": 577,\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"processor_class\": \"Florence2Processor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 768,\n",
       "    \"width\": 768\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: BartTokenizerFast(name_or_path='microsoft/Florence-2-base', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['<od>', '</od>', '<ocr>', '</ocr>', '<loc_0>', '<loc_1>', '<loc_2>', '<loc_3>', '<loc_4>', '<loc_5>', '<loc_6>', '<loc_7>', '<loc_8>', '<loc_9>', '<loc_10>', '<loc_11>', '<loc_12>', '<loc_13>', '<loc_14>', '<loc_15>', '<loc_16>', '<loc_17>', '<loc_18>', '<loc_19>', '<loc_20>', '<loc_21>', '<loc_22>', '<loc_23>', '<loc_24>', '<loc_25>', '<loc_26>', '<loc_27>', '<loc_28>', '<loc_29>', '<loc_30>', '<loc_31>', '<loc_32>', '<loc_33>', '<loc_34>', '<loc_35>', '<loc_36>', '<loc_37>', '<loc_38>', '<loc_39>', '<loc_40>', '<loc_41>', '<loc_42>', '<loc_43>', '<loc_44>', '<loc_45>', '<loc_46>', '<loc_47>', '<loc_48>', '<loc_49>', '<loc_50>', '<loc_51>', '<loc_52>', '<loc_53>', '<loc_54>', '<loc_55>', '<loc_56>', '<loc_57>', '<loc_58>', '<loc_59>', '<loc_60>', '<loc_61>', '<loc_62>', '<loc_63>', '<loc_64>', '<loc_65>', '<loc_66>', '<loc_67>', '<loc_68>', '<loc_69>', '<loc_70>', '<loc_71>', '<loc_72>', '<loc_73>', '<loc_74>', '<loc_75>', '<loc_76>', '<loc_77>', '<loc_78>', '<loc_79>', '<loc_80>', '<loc_81>', '<loc_82>', '<loc_83>', '<loc_84>', '<loc_85>', '<loc_86>', '<loc_87>', '<loc_88>', '<loc_89>', '<loc_90>', '<loc_91>', '<loc_92>', '<loc_93>', '<loc_94>', '<loc_95>', '<loc_96>', '<loc_97>', '<loc_98>', '<loc_99>', '<loc_100>', '<loc_101>', '<loc_102>', '<loc_103>', '<loc_104>', '<loc_105>', '<loc_106>', '<loc_107>', '<loc_108>', '<loc_109>', '<loc_110>', '<loc_111>', '<loc_112>', '<loc_113>', '<loc_114>', '<loc_115>', '<loc_116>', '<loc_117>', '<loc_118>', '<loc_119>', '<loc_120>', '<loc_121>', '<loc_122>', '<loc_123>', '<loc_124>', '<loc_125>', '<loc_126>', '<loc_127>', '<loc_128>', '<loc_129>', '<loc_130>', '<loc_131>', '<loc_132>', '<loc_133>', '<loc_134>', '<loc_135>', '<loc_136>', '<loc_137>', '<loc_138>', '<loc_139>', '<loc_140>', '<loc_141>', '<loc_142>', '<loc_143>', '<loc_144>', '<loc_145>', '<loc_146>', '<loc_147>', '<loc_148>', '<loc_149>', '<loc_150>', '<loc_151>', '<loc_152>', '<loc_153>', '<loc_154>', '<loc_155>', '<loc_156>', '<loc_157>', '<loc_158>', '<loc_159>', '<loc_160>', '<loc_161>', '<loc_162>', '<loc_163>', '<loc_164>', '<loc_165>', '<loc_166>', '<loc_167>', '<loc_168>', '<loc_169>', '<loc_170>', '<loc_171>', '<loc_172>', '<loc_173>', '<loc_174>', '<loc_175>', '<loc_176>', '<loc_177>', '<loc_178>', '<loc_179>', '<loc_180>', '<loc_181>', '<loc_182>', '<loc_183>', '<loc_184>', '<loc_185>', '<loc_186>', '<loc_187>', '<loc_188>', '<loc_189>', '<loc_190>', '<loc_191>', '<loc_192>', '<loc_193>', '<loc_194>', '<loc_195>', '<loc_196>', '<loc_197>', '<loc_198>', '<loc_199>', '<loc_200>', '<loc_201>', '<loc_202>', '<loc_203>', '<loc_204>', '<loc_205>', '<loc_206>', '<loc_207>', '<loc_208>', '<loc_209>', '<loc_210>', '<loc_211>', '<loc_212>', '<loc_213>', '<loc_214>', '<loc_215>', '<loc_216>', '<loc_217>', '<loc_218>', '<loc_219>', '<loc_220>', '<loc_221>', '<loc_222>', '<loc_223>', '<loc_224>', '<loc_225>', '<loc_226>', '<loc_227>', '<loc_228>', '<loc_229>', '<loc_230>', '<loc_231>', '<loc_232>', '<loc_233>', '<loc_234>', '<loc_235>', '<loc_236>', '<loc_237>', '<loc_238>', '<loc_239>', '<loc_240>', '<loc_241>', '<loc_242>', '<loc_243>', '<loc_244>', '<loc_245>', '<loc_246>', '<loc_247>', '<loc_248>', '<loc_249>', '<loc_250>', '<loc_251>', '<loc_252>', '<loc_253>', '<loc_254>', '<loc_255>', '<loc_256>', '<loc_257>', '<loc_258>', '<loc_259>', '<loc_260>', '<loc_261>', '<loc_262>', '<loc_263>', '<loc_264>', '<loc_265>', '<loc_266>', '<loc_267>', '<loc_268>', '<loc_269>', '<loc_270>', '<loc_271>', '<loc_272>', '<loc_273>', '<loc_274>', '<loc_275>', '<loc_276>', '<loc_277>', '<loc_278>', '<loc_279>', '<loc_280>', '<loc_281>', '<loc_282>', '<loc_283>', '<loc_284>', '<loc_285>', '<loc_286>', '<loc_287>', '<loc_288>', '<loc_289>', '<loc_290>', '<loc_291>', '<loc_292>', '<loc_293>', '<loc_294>', '<loc_295>', '<loc_296>', '<loc_297>', '<loc_298>', '<loc_299>', '<loc_300>', '<loc_301>', '<loc_302>', '<loc_303>', '<loc_304>', '<loc_305>', '<loc_306>', '<loc_307>', '<loc_308>', '<loc_309>', '<loc_310>', '<loc_311>', '<loc_312>', '<loc_313>', '<loc_314>', '<loc_315>', '<loc_316>', '<loc_317>', '<loc_318>', '<loc_319>', '<loc_320>', '<loc_321>', '<loc_322>', '<loc_323>', '<loc_324>', '<loc_325>', '<loc_326>', '<loc_327>', '<loc_328>', '<loc_329>', '<loc_330>', '<loc_331>', '<loc_332>', '<loc_333>', '<loc_334>', '<loc_335>', '<loc_336>', '<loc_337>', '<loc_338>', '<loc_339>', '<loc_340>', '<loc_341>', '<loc_342>', '<loc_343>', '<loc_344>', '<loc_345>', '<loc_346>', '<loc_347>', '<loc_348>', '<loc_349>', '<loc_350>', '<loc_351>', '<loc_352>', '<loc_353>', '<loc_354>', '<loc_355>', '<loc_356>', '<loc_357>', '<loc_358>', '<loc_359>', '<loc_360>', '<loc_361>', '<loc_362>', '<loc_363>', '<loc_364>', '<loc_365>', '<loc_366>', '<loc_367>', '<loc_368>', '<loc_369>', '<loc_370>', '<loc_371>', '<loc_372>', '<loc_373>', '<loc_374>', '<loc_375>', '<loc_376>', '<loc_377>', '<loc_378>', '<loc_379>', '<loc_380>', '<loc_381>', '<loc_382>', '<loc_383>', '<loc_384>', '<loc_385>', '<loc_386>', '<loc_387>', '<loc_388>', '<loc_389>', '<loc_390>', '<loc_391>', '<loc_392>', '<loc_393>', '<loc_394>', '<loc_395>', '<loc_396>', '<loc_397>', '<loc_398>', '<loc_399>', '<loc_400>', '<loc_401>', '<loc_402>', '<loc_403>', '<loc_404>', '<loc_405>', '<loc_406>', '<loc_407>', '<loc_408>', '<loc_409>', '<loc_410>', '<loc_411>', '<loc_412>', '<loc_413>', '<loc_414>', '<loc_415>', '<loc_416>', '<loc_417>', '<loc_418>', '<loc_419>', '<loc_420>', '<loc_421>', '<loc_422>', '<loc_423>', '<loc_424>', '<loc_425>', '<loc_426>', '<loc_427>', '<loc_428>', '<loc_429>', '<loc_430>', '<loc_431>', '<loc_432>', '<loc_433>', '<loc_434>', '<loc_435>', '<loc_436>', '<loc_437>', '<loc_438>', '<loc_439>', '<loc_440>', '<loc_441>', '<loc_442>', '<loc_443>', '<loc_444>', '<loc_445>', '<loc_446>', '<loc_447>', '<loc_448>', '<loc_449>', '<loc_450>', '<loc_451>', '<loc_452>', '<loc_453>', '<loc_454>', '<loc_455>', '<loc_456>', '<loc_457>', '<loc_458>', '<loc_459>', '<loc_460>', '<loc_461>', '<loc_462>', '<loc_463>', '<loc_464>', '<loc_465>', '<loc_466>', '<loc_467>', '<loc_468>', '<loc_469>', '<loc_470>', '<loc_471>', '<loc_472>', '<loc_473>', '<loc_474>', '<loc_475>', '<loc_476>', '<loc_477>', '<loc_478>', '<loc_479>', '<loc_480>', '<loc_481>', '<loc_482>', '<loc_483>', '<loc_484>', '<loc_485>', '<loc_486>', '<loc_487>', '<loc_488>', '<loc_489>', '<loc_490>', '<loc_491>', '<loc_492>', '<loc_493>', '<loc_494>', '<loc_495>', '<loc_496>', '<loc_497>', '<loc_498>', '<loc_499>', '<loc_500>', '<loc_501>', '<loc_502>', '<loc_503>', '<loc_504>', '<loc_505>', '<loc_506>', '<loc_507>', '<loc_508>', '<loc_509>', '<loc_510>', '<loc_511>', '<loc_512>', '<loc_513>', '<loc_514>', '<loc_515>', '<loc_516>', '<loc_517>', '<loc_518>', '<loc_519>', '<loc_520>', '<loc_521>', '<loc_522>', '<loc_523>', '<loc_524>', '<loc_525>', '<loc_526>', '<loc_527>', '<loc_528>', '<loc_529>', '<loc_530>', '<loc_531>', '<loc_532>', '<loc_533>', '<loc_534>', '<loc_535>', '<loc_536>', '<loc_537>', '<loc_538>', '<loc_539>', '<loc_540>', '<loc_541>', '<loc_542>', '<loc_543>', '<loc_544>', '<loc_545>', '<loc_546>', '<loc_547>', '<loc_548>', '<loc_549>', '<loc_550>', '<loc_551>', '<loc_552>', '<loc_553>', '<loc_554>', '<loc_555>', '<loc_556>', '<loc_557>', '<loc_558>', '<loc_559>', '<loc_560>', '<loc_561>', '<loc_562>', '<loc_563>', '<loc_564>', '<loc_565>', '<loc_566>', '<loc_567>', '<loc_568>', '<loc_569>', '<loc_570>', '<loc_571>', '<loc_572>', '<loc_573>', '<loc_574>', '<loc_575>', '<loc_576>', '<loc_577>', '<loc_578>', '<loc_579>', '<loc_580>', '<loc_581>', '<loc_582>', '<loc_583>', '<loc_584>', '<loc_585>', '<loc_586>', '<loc_587>', '<loc_588>', '<loc_589>', '<loc_590>', '<loc_591>', '<loc_592>', '<loc_593>', '<loc_594>', '<loc_595>', '<loc_596>', '<loc_597>', '<loc_598>', '<loc_599>', '<loc_600>', '<loc_601>', '<loc_602>', '<loc_603>', '<loc_604>', '<loc_605>', '<loc_606>', '<loc_607>', '<loc_608>', '<loc_609>', '<loc_610>', '<loc_611>', '<loc_612>', '<loc_613>', '<loc_614>', '<loc_615>', '<loc_616>', '<loc_617>', '<loc_618>', '<loc_619>', '<loc_620>', '<loc_621>', '<loc_622>', '<loc_623>', '<loc_624>', '<loc_625>', '<loc_626>', '<loc_627>', '<loc_628>', '<loc_629>', '<loc_630>', '<loc_631>', '<loc_632>', '<loc_633>', '<loc_634>', '<loc_635>', '<loc_636>', '<loc_637>', '<loc_638>', '<loc_639>', '<loc_640>', '<loc_641>', '<loc_642>', '<loc_643>', '<loc_644>', '<loc_645>', '<loc_646>', '<loc_647>', '<loc_648>', '<loc_649>', '<loc_650>', '<loc_651>', '<loc_652>', '<loc_653>', '<loc_654>', '<loc_655>', '<loc_656>', '<loc_657>', '<loc_658>', '<loc_659>', '<loc_660>', '<loc_661>', '<loc_662>', '<loc_663>', '<loc_664>', '<loc_665>', '<loc_666>', '<loc_667>', '<loc_668>', '<loc_669>', '<loc_670>', '<loc_671>', '<loc_672>', '<loc_673>', '<loc_674>', '<loc_675>', '<loc_676>', '<loc_677>', '<loc_678>', '<loc_679>', '<loc_680>', '<loc_681>', '<loc_682>', '<loc_683>', '<loc_684>', '<loc_685>', '<loc_686>', '<loc_687>', '<loc_688>', '<loc_689>', '<loc_690>', '<loc_691>', '<loc_692>', '<loc_693>', '<loc_694>', '<loc_695>', '<loc_696>', '<loc_697>', '<loc_698>', '<loc_699>', '<loc_700>', '<loc_701>', '<loc_702>', '<loc_703>', '<loc_704>', '<loc_705>', '<loc_706>', '<loc_707>', '<loc_708>', '<loc_709>', '<loc_710>', '<loc_711>', '<loc_712>', '<loc_713>', '<loc_714>', '<loc_715>', '<loc_716>', '<loc_717>', '<loc_718>', '<loc_719>', '<loc_720>', '<loc_721>', '<loc_722>', '<loc_723>', '<loc_724>', '<loc_725>', '<loc_726>', '<loc_727>', '<loc_728>', '<loc_729>', '<loc_730>', '<loc_731>', '<loc_732>', '<loc_733>', '<loc_734>', '<loc_735>', '<loc_736>', '<loc_737>', '<loc_738>', '<loc_739>', '<loc_740>', '<loc_741>', '<loc_742>', '<loc_743>', '<loc_744>', '<loc_745>', '<loc_746>', '<loc_747>', '<loc_748>', '<loc_749>', '<loc_750>', '<loc_751>', '<loc_752>', '<loc_753>', '<loc_754>', '<loc_755>', '<loc_756>', '<loc_757>', '<loc_758>', '<loc_759>', '<loc_760>', '<loc_761>', '<loc_762>', '<loc_763>', '<loc_764>', '<loc_765>', '<loc_766>', '<loc_767>', '<loc_768>', '<loc_769>', '<loc_770>', '<loc_771>', '<loc_772>', '<loc_773>', '<loc_774>', '<loc_775>', '<loc_776>', '<loc_777>', '<loc_778>', '<loc_779>', '<loc_780>', '<loc_781>', '<loc_782>', '<loc_783>', '<loc_784>', '<loc_785>', '<loc_786>', '<loc_787>', '<loc_788>', '<loc_789>', '<loc_790>', '<loc_791>', '<loc_792>', '<loc_793>', '<loc_794>', '<loc_795>', '<loc_796>', '<loc_797>', '<loc_798>', '<loc_799>', '<loc_800>', '<loc_801>', '<loc_802>', '<loc_803>', '<loc_804>', '<loc_805>', '<loc_806>', '<loc_807>', '<loc_808>', '<loc_809>', '<loc_810>', '<loc_811>', '<loc_812>', '<loc_813>', '<loc_814>', '<loc_815>', '<loc_816>', '<loc_817>', '<loc_818>', '<loc_819>', '<loc_820>', '<loc_821>', '<loc_822>', '<loc_823>', '<loc_824>', '<loc_825>', '<loc_826>', '<loc_827>', '<loc_828>', '<loc_829>', '<loc_830>', '<loc_831>', '<loc_832>', '<loc_833>', '<loc_834>', '<loc_835>', '<loc_836>', '<loc_837>', '<loc_838>', '<loc_839>', '<loc_840>', '<loc_841>', '<loc_842>', '<loc_843>', '<loc_844>', '<loc_845>', '<loc_846>', '<loc_847>', '<loc_848>', '<loc_849>', '<loc_850>', '<loc_851>', '<loc_852>', '<loc_853>', '<loc_854>', '<loc_855>', '<loc_856>', '<loc_857>', '<loc_858>', '<loc_859>', '<loc_860>', '<loc_861>', '<loc_862>', '<loc_863>', '<loc_864>', '<loc_865>', '<loc_866>', '<loc_867>', '<loc_868>', '<loc_869>', '<loc_870>', '<loc_871>', '<loc_872>', '<loc_873>', '<loc_874>', '<loc_875>', '<loc_876>', '<loc_877>', '<loc_878>', '<loc_879>', '<loc_880>', '<loc_881>', '<loc_882>', '<loc_883>', '<loc_884>', '<loc_885>', '<loc_886>', '<loc_887>', '<loc_888>', '<loc_889>', '<loc_890>', '<loc_891>', '<loc_892>', '<loc_893>', '<loc_894>', '<loc_895>', '<loc_896>', '<loc_897>', '<loc_898>', '<loc_899>', '<loc_900>', '<loc_901>', '<loc_902>', '<loc_903>', '<loc_904>', '<loc_905>', '<loc_906>', '<loc_907>', '<loc_908>', '<loc_909>', '<loc_910>', '<loc_911>', '<loc_912>', '<loc_913>', '<loc_914>', '<loc_915>', '<loc_916>', '<loc_917>', '<loc_918>', '<loc_919>', '<loc_920>', '<loc_921>', '<loc_922>', '<loc_923>', '<loc_924>', '<loc_925>', '<loc_926>', '<loc_927>', '<loc_928>', '<loc_929>', '<loc_930>', '<loc_931>', '<loc_932>', '<loc_933>', '<loc_934>', '<loc_935>', '<loc_936>', '<loc_937>', '<loc_938>', '<loc_939>', '<loc_940>', '<loc_941>', '<loc_942>', '<loc_943>', '<loc_944>', '<loc_945>', '<loc_946>', '<loc_947>', '<loc_948>', '<loc_949>', '<loc_950>', '<loc_951>', '<loc_952>', '<loc_953>', '<loc_954>', '<loc_955>', '<loc_956>', '<loc_957>', '<loc_958>', '<loc_959>', '<loc_960>', '<loc_961>', '<loc_962>', '<loc_963>', '<loc_964>', '<loc_965>', '<loc_966>', '<loc_967>', '<loc_968>', '<loc_969>', '<loc_970>', '<loc_971>', '<loc_972>', '<loc_973>', '<loc_974>', '<loc_975>', '<loc_976>', '<loc_977>', '<loc_978>', '<loc_979>', '<loc_980>', '<loc_981>', '<loc_982>', '<loc_983>', '<loc_984>', '<loc_985>', '<loc_986>', '<loc_987>', '<loc_988>', '<loc_989>', '<loc_990>', '<loc_991>', '<loc_992>', '<loc_993>', '<loc_994>', '<loc_995>', '<loc_996>', '<loc_997>', '<loc_998>', '<loc_999>', '<cap>', '</cap>', '<ncap>', '</ncap>', '<dcap>', '</dcap>', '<grounding>', '</grounding>', '<seg>', '</seg>', '<sep>', '<region_cap>', '</region_cap>', '<region_to_desciption>', '</region_to_desciption>', '<proposal>', '</proposal>', '<poly>', '</poly>', '<and>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "\t50265: AddedToken(\"<od>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50266: AddedToken(\"</od>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50267: AddedToken(\"<ocr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50268: AddedToken(\"</ocr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50269: AddedToken(\"<loc_0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50270: AddedToken(\"<loc_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50271: AddedToken(\"<loc_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50272: AddedToken(\"<loc_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50273: AddedToken(\"<loc_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50274: AddedToken(\"<loc_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50275: AddedToken(\"<loc_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50276: AddedToken(\"<loc_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50277: AddedToken(\"<loc_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50278: AddedToken(\"<loc_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50279: AddedToken(\"<loc_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50280: AddedToken(\"<loc_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50281: AddedToken(\"<loc_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50282: AddedToken(\"<loc_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50283: AddedToken(\"<loc_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50284: AddedToken(\"<loc_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50285: AddedToken(\"<loc_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50286: AddedToken(\"<loc_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50287: AddedToken(\"<loc_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50288: AddedToken(\"<loc_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50289: AddedToken(\"<loc_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50290: AddedToken(\"<loc_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50291: AddedToken(\"<loc_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50292: AddedToken(\"<loc_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50293: AddedToken(\"<loc_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50294: AddedToken(\"<loc_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50295: AddedToken(\"<loc_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50296: AddedToken(\"<loc_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50297: AddedToken(\"<loc_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50298: AddedToken(\"<loc_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50299: AddedToken(\"<loc_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50300: AddedToken(\"<loc_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50301: AddedToken(\"<loc_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50302: AddedToken(\"<loc_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50303: AddedToken(\"<loc_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50304: AddedToken(\"<loc_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50305: AddedToken(\"<loc_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50306: AddedToken(\"<loc_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50307: AddedToken(\"<loc_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50308: AddedToken(\"<loc_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50309: AddedToken(\"<loc_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50310: AddedToken(\"<loc_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50311: AddedToken(\"<loc_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50312: AddedToken(\"<loc_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50313: AddedToken(\"<loc_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50314: AddedToken(\"<loc_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50315: AddedToken(\"<loc_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50316: AddedToken(\"<loc_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50317: AddedToken(\"<loc_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50318: AddedToken(\"<loc_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50319: AddedToken(\"<loc_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50320: AddedToken(\"<loc_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50321: AddedToken(\"<loc_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50322: AddedToken(\"<loc_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50323: AddedToken(\"<loc_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50324: AddedToken(\"<loc_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50325: AddedToken(\"<loc_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50326: AddedToken(\"<loc_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50327: AddedToken(\"<loc_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50328: AddedToken(\"<loc_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50329: AddedToken(\"<loc_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50330: AddedToken(\"<loc_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50331: AddedToken(\"<loc_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50332: AddedToken(\"<loc_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50333: AddedToken(\"<loc_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50334: AddedToken(\"<loc_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50335: AddedToken(\"<loc_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50336: AddedToken(\"<loc_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50337: AddedToken(\"<loc_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50338: AddedToken(\"<loc_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50339: AddedToken(\"<loc_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50340: AddedToken(\"<loc_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50341: AddedToken(\"<loc_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50342: AddedToken(\"<loc_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50343: AddedToken(\"<loc_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50344: AddedToken(\"<loc_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50345: AddedToken(\"<loc_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50346: AddedToken(\"<loc_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50347: AddedToken(\"<loc_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50348: AddedToken(\"<loc_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50349: AddedToken(\"<loc_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50350: AddedToken(\"<loc_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50351: AddedToken(\"<loc_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50352: AddedToken(\"<loc_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50353: AddedToken(\"<loc_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50354: AddedToken(\"<loc_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50355: AddedToken(\"<loc_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50356: AddedToken(\"<loc_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50357: AddedToken(\"<loc_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50358: AddedToken(\"<loc_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50359: AddedToken(\"<loc_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50360: AddedToken(\"<loc_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50361: AddedToken(\"<loc_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50362: AddedToken(\"<loc_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50363: AddedToken(\"<loc_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50364: AddedToken(\"<loc_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50365: AddedToken(\"<loc_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50366: AddedToken(\"<loc_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50367: AddedToken(\"<loc_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50368: AddedToken(\"<loc_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50369: AddedToken(\"<loc_100>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50370: AddedToken(\"<loc_101>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50371: AddedToken(\"<loc_102>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50372: AddedToken(\"<loc_103>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50373: AddedToken(\"<loc_104>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50374: AddedToken(\"<loc_105>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50375: AddedToken(\"<loc_106>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50376: AddedToken(\"<loc_107>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50377: AddedToken(\"<loc_108>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50378: AddedToken(\"<loc_109>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50379: AddedToken(\"<loc_110>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50380: AddedToken(\"<loc_111>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50381: AddedToken(\"<loc_112>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50382: AddedToken(\"<loc_113>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50383: AddedToken(\"<loc_114>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50384: AddedToken(\"<loc_115>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50385: AddedToken(\"<loc_116>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50386: AddedToken(\"<loc_117>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50387: AddedToken(\"<loc_118>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50388: AddedToken(\"<loc_119>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50389: AddedToken(\"<loc_120>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50390: AddedToken(\"<loc_121>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50391: AddedToken(\"<loc_122>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50392: AddedToken(\"<loc_123>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50393: AddedToken(\"<loc_124>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50394: AddedToken(\"<loc_125>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50395: AddedToken(\"<loc_126>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50396: AddedToken(\"<loc_127>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50397: AddedToken(\"<loc_128>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50398: AddedToken(\"<loc_129>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50399: AddedToken(\"<loc_130>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50400: AddedToken(\"<loc_131>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50401: AddedToken(\"<loc_132>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50402: AddedToken(\"<loc_133>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50403: AddedToken(\"<loc_134>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50404: AddedToken(\"<loc_135>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50405: AddedToken(\"<loc_136>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50406: AddedToken(\"<loc_137>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50407: AddedToken(\"<loc_138>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50408: AddedToken(\"<loc_139>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50409: AddedToken(\"<loc_140>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50410: AddedToken(\"<loc_141>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50411: AddedToken(\"<loc_142>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50412: AddedToken(\"<loc_143>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50413: AddedToken(\"<loc_144>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50414: AddedToken(\"<loc_145>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50415: AddedToken(\"<loc_146>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50416: AddedToken(\"<loc_147>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50417: AddedToken(\"<loc_148>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50418: AddedToken(\"<loc_149>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50419: AddedToken(\"<loc_150>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50420: AddedToken(\"<loc_151>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50421: AddedToken(\"<loc_152>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50422: AddedToken(\"<loc_153>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50423: AddedToken(\"<loc_154>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50424: AddedToken(\"<loc_155>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50425: AddedToken(\"<loc_156>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50426: AddedToken(\"<loc_157>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50427: AddedToken(\"<loc_158>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50428: AddedToken(\"<loc_159>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50429: AddedToken(\"<loc_160>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50430: AddedToken(\"<loc_161>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50431: AddedToken(\"<loc_162>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50432: AddedToken(\"<loc_163>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50433: AddedToken(\"<loc_164>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50434: AddedToken(\"<loc_165>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50435: AddedToken(\"<loc_166>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50436: AddedToken(\"<loc_167>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50437: AddedToken(\"<loc_168>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50438: AddedToken(\"<loc_169>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50439: AddedToken(\"<loc_170>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50440: AddedToken(\"<loc_171>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50441: AddedToken(\"<loc_172>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50442: AddedToken(\"<loc_173>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50443: AddedToken(\"<loc_174>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50444: AddedToken(\"<loc_175>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50445: AddedToken(\"<loc_176>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50446: AddedToken(\"<loc_177>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50447: AddedToken(\"<loc_178>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50448: AddedToken(\"<loc_179>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50449: AddedToken(\"<loc_180>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50450: AddedToken(\"<loc_181>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50451: AddedToken(\"<loc_182>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50452: AddedToken(\"<loc_183>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50453: AddedToken(\"<loc_184>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50454: AddedToken(\"<loc_185>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50455: AddedToken(\"<loc_186>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50456: AddedToken(\"<loc_187>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50457: AddedToken(\"<loc_188>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50458: AddedToken(\"<loc_189>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50459: AddedToken(\"<loc_190>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50460: AddedToken(\"<loc_191>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50461: AddedToken(\"<loc_192>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50462: AddedToken(\"<loc_193>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50463: AddedToken(\"<loc_194>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50464: AddedToken(\"<loc_195>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50465: AddedToken(\"<loc_196>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50466: AddedToken(\"<loc_197>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50467: AddedToken(\"<loc_198>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50468: AddedToken(\"<loc_199>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50469: AddedToken(\"<loc_200>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50470: AddedToken(\"<loc_201>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50471: AddedToken(\"<loc_202>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50472: AddedToken(\"<loc_203>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50473: AddedToken(\"<loc_204>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50474: AddedToken(\"<loc_205>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50475: AddedToken(\"<loc_206>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50476: AddedToken(\"<loc_207>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50477: AddedToken(\"<loc_208>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50478: AddedToken(\"<loc_209>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50479: AddedToken(\"<loc_210>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50480: AddedToken(\"<loc_211>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50481: AddedToken(\"<loc_212>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50482: AddedToken(\"<loc_213>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50483: AddedToken(\"<loc_214>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50484: AddedToken(\"<loc_215>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50485: AddedToken(\"<loc_216>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50486: AddedToken(\"<loc_217>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50487: AddedToken(\"<loc_218>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50488: AddedToken(\"<loc_219>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50489: AddedToken(\"<loc_220>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50490: AddedToken(\"<loc_221>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50491: AddedToken(\"<loc_222>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50492: AddedToken(\"<loc_223>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50493: AddedToken(\"<loc_224>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50494: AddedToken(\"<loc_225>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50495: AddedToken(\"<loc_226>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50496: AddedToken(\"<loc_227>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50497: AddedToken(\"<loc_228>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50498: AddedToken(\"<loc_229>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50499: AddedToken(\"<loc_230>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50500: AddedToken(\"<loc_231>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50501: AddedToken(\"<loc_232>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50502: AddedToken(\"<loc_233>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50503: AddedToken(\"<loc_234>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50504: AddedToken(\"<loc_235>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50505: AddedToken(\"<loc_236>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50506: AddedToken(\"<loc_237>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50507: AddedToken(\"<loc_238>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50508: AddedToken(\"<loc_239>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50509: AddedToken(\"<loc_240>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50510: AddedToken(\"<loc_241>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50511: AddedToken(\"<loc_242>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50512: AddedToken(\"<loc_243>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50513: AddedToken(\"<loc_244>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50514: AddedToken(\"<loc_245>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50515: AddedToken(\"<loc_246>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50516: AddedToken(\"<loc_247>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50517: AddedToken(\"<loc_248>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50518: AddedToken(\"<loc_249>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50519: AddedToken(\"<loc_250>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50520: AddedToken(\"<loc_251>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50521: AddedToken(\"<loc_252>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50522: AddedToken(\"<loc_253>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50523: AddedToken(\"<loc_254>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50524: AddedToken(\"<loc_255>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50525: AddedToken(\"<loc_256>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50526: AddedToken(\"<loc_257>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50527: AddedToken(\"<loc_258>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50528: AddedToken(\"<loc_259>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50529: AddedToken(\"<loc_260>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50530: AddedToken(\"<loc_261>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50531: AddedToken(\"<loc_262>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50532: AddedToken(\"<loc_263>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50533: AddedToken(\"<loc_264>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50534: AddedToken(\"<loc_265>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50535: AddedToken(\"<loc_266>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50536: AddedToken(\"<loc_267>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50537: AddedToken(\"<loc_268>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50538: AddedToken(\"<loc_269>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50539: AddedToken(\"<loc_270>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50540: AddedToken(\"<loc_271>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50541: AddedToken(\"<loc_272>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50542: AddedToken(\"<loc_273>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50543: AddedToken(\"<loc_274>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50544: AddedToken(\"<loc_275>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50545: AddedToken(\"<loc_276>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50546: AddedToken(\"<loc_277>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50547: AddedToken(\"<loc_278>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50548: AddedToken(\"<loc_279>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50549: AddedToken(\"<loc_280>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50550: AddedToken(\"<loc_281>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50551: AddedToken(\"<loc_282>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50552: AddedToken(\"<loc_283>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50553: AddedToken(\"<loc_284>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50554: AddedToken(\"<loc_285>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50555: AddedToken(\"<loc_286>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50556: AddedToken(\"<loc_287>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50557: AddedToken(\"<loc_288>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50558: AddedToken(\"<loc_289>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50559: AddedToken(\"<loc_290>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50560: AddedToken(\"<loc_291>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50561: AddedToken(\"<loc_292>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50562: AddedToken(\"<loc_293>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50563: AddedToken(\"<loc_294>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50564: AddedToken(\"<loc_295>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50565: AddedToken(\"<loc_296>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50566: AddedToken(\"<loc_297>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50567: AddedToken(\"<loc_298>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50568: AddedToken(\"<loc_299>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50569: AddedToken(\"<loc_300>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50570: AddedToken(\"<loc_301>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50571: AddedToken(\"<loc_302>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50572: AddedToken(\"<loc_303>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50573: AddedToken(\"<loc_304>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50574: AddedToken(\"<loc_305>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50575: AddedToken(\"<loc_306>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50576: AddedToken(\"<loc_307>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50577: AddedToken(\"<loc_308>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50578: AddedToken(\"<loc_309>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50579: AddedToken(\"<loc_310>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50580: AddedToken(\"<loc_311>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50581: AddedToken(\"<loc_312>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50582: AddedToken(\"<loc_313>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50583: AddedToken(\"<loc_314>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50584: AddedToken(\"<loc_315>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50585: AddedToken(\"<loc_316>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50586: AddedToken(\"<loc_317>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50587: AddedToken(\"<loc_318>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50588: AddedToken(\"<loc_319>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50589: AddedToken(\"<loc_320>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50590: AddedToken(\"<loc_321>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50591: AddedToken(\"<loc_322>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50592: AddedToken(\"<loc_323>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50593: AddedToken(\"<loc_324>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50594: AddedToken(\"<loc_325>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50595: AddedToken(\"<loc_326>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50596: AddedToken(\"<loc_327>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50597: AddedToken(\"<loc_328>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50598: AddedToken(\"<loc_329>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50599: AddedToken(\"<loc_330>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50600: AddedToken(\"<loc_331>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50601: AddedToken(\"<loc_332>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50602: AddedToken(\"<loc_333>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50603: AddedToken(\"<loc_334>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50604: AddedToken(\"<loc_335>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50605: AddedToken(\"<loc_336>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50606: AddedToken(\"<loc_337>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50607: AddedToken(\"<loc_338>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50608: AddedToken(\"<loc_339>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50609: AddedToken(\"<loc_340>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50610: AddedToken(\"<loc_341>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50611: AddedToken(\"<loc_342>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50612: AddedToken(\"<loc_343>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50613: AddedToken(\"<loc_344>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50614: AddedToken(\"<loc_345>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50615: AddedToken(\"<loc_346>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50616: AddedToken(\"<loc_347>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50617: AddedToken(\"<loc_348>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50618: AddedToken(\"<loc_349>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50619: AddedToken(\"<loc_350>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50620: AddedToken(\"<loc_351>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50621: AddedToken(\"<loc_352>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50622: AddedToken(\"<loc_353>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50623: AddedToken(\"<loc_354>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50624: AddedToken(\"<loc_355>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50625: AddedToken(\"<loc_356>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50626: AddedToken(\"<loc_357>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50627: AddedToken(\"<loc_358>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50628: AddedToken(\"<loc_359>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50629: AddedToken(\"<loc_360>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50630: AddedToken(\"<loc_361>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50631: AddedToken(\"<loc_362>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50632: AddedToken(\"<loc_363>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50633: AddedToken(\"<loc_364>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50634: AddedToken(\"<loc_365>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50635: AddedToken(\"<loc_366>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50636: AddedToken(\"<loc_367>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50637: AddedToken(\"<loc_368>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50638: AddedToken(\"<loc_369>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50639: AddedToken(\"<loc_370>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50640: AddedToken(\"<loc_371>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50641: AddedToken(\"<loc_372>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50642: AddedToken(\"<loc_373>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50643: AddedToken(\"<loc_374>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50644: AddedToken(\"<loc_375>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50645: AddedToken(\"<loc_376>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50646: AddedToken(\"<loc_377>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50647: AddedToken(\"<loc_378>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50648: AddedToken(\"<loc_379>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50649: AddedToken(\"<loc_380>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50650: AddedToken(\"<loc_381>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50651: AddedToken(\"<loc_382>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50652: AddedToken(\"<loc_383>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50653: AddedToken(\"<loc_384>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50654: AddedToken(\"<loc_385>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50655: AddedToken(\"<loc_386>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50656: AddedToken(\"<loc_387>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50657: AddedToken(\"<loc_388>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50658: AddedToken(\"<loc_389>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50659: AddedToken(\"<loc_390>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50660: AddedToken(\"<loc_391>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50661: AddedToken(\"<loc_392>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50662: AddedToken(\"<loc_393>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50663: AddedToken(\"<loc_394>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50664: AddedToken(\"<loc_395>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50665: AddedToken(\"<loc_396>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50666: AddedToken(\"<loc_397>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50667: AddedToken(\"<loc_398>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50668: AddedToken(\"<loc_399>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50669: AddedToken(\"<loc_400>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50670: AddedToken(\"<loc_401>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50671: AddedToken(\"<loc_402>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50672: AddedToken(\"<loc_403>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50673: AddedToken(\"<loc_404>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50674: AddedToken(\"<loc_405>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50675: AddedToken(\"<loc_406>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50676: AddedToken(\"<loc_407>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50677: AddedToken(\"<loc_408>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50678: AddedToken(\"<loc_409>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50679: AddedToken(\"<loc_410>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50680: AddedToken(\"<loc_411>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50681: AddedToken(\"<loc_412>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50682: AddedToken(\"<loc_413>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50683: AddedToken(\"<loc_414>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50684: AddedToken(\"<loc_415>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50685: AddedToken(\"<loc_416>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50686: AddedToken(\"<loc_417>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50687: AddedToken(\"<loc_418>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50688: AddedToken(\"<loc_419>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50689: AddedToken(\"<loc_420>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50690: AddedToken(\"<loc_421>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50691: AddedToken(\"<loc_422>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50692: AddedToken(\"<loc_423>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50693: AddedToken(\"<loc_424>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50694: AddedToken(\"<loc_425>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50695: AddedToken(\"<loc_426>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50696: AddedToken(\"<loc_427>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50697: AddedToken(\"<loc_428>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50698: AddedToken(\"<loc_429>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50699: AddedToken(\"<loc_430>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50700: AddedToken(\"<loc_431>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50701: AddedToken(\"<loc_432>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50702: AddedToken(\"<loc_433>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50703: AddedToken(\"<loc_434>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50704: AddedToken(\"<loc_435>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50705: AddedToken(\"<loc_436>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50706: AddedToken(\"<loc_437>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50707: AddedToken(\"<loc_438>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50708: AddedToken(\"<loc_439>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50709: AddedToken(\"<loc_440>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50710: AddedToken(\"<loc_441>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50711: AddedToken(\"<loc_442>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50712: AddedToken(\"<loc_443>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50713: AddedToken(\"<loc_444>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50714: AddedToken(\"<loc_445>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50715: AddedToken(\"<loc_446>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50716: AddedToken(\"<loc_447>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50717: AddedToken(\"<loc_448>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50718: AddedToken(\"<loc_449>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50719: AddedToken(\"<loc_450>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50720: AddedToken(\"<loc_451>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50721: AddedToken(\"<loc_452>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50722: AddedToken(\"<loc_453>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50723: AddedToken(\"<loc_454>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50724: AddedToken(\"<loc_455>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50725: AddedToken(\"<loc_456>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50726: AddedToken(\"<loc_457>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50727: AddedToken(\"<loc_458>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50728: AddedToken(\"<loc_459>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50729: AddedToken(\"<loc_460>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50730: AddedToken(\"<loc_461>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50731: AddedToken(\"<loc_462>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50732: AddedToken(\"<loc_463>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50733: AddedToken(\"<loc_464>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50734: AddedToken(\"<loc_465>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50735: AddedToken(\"<loc_466>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50736: AddedToken(\"<loc_467>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50737: AddedToken(\"<loc_468>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50738: AddedToken(\"<loc_469>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50739: AddedToken(\"<loc_470>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50740: AddedToken(\"<loc_471>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50741: AddedToken(\"<loc_472>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50742: AddedToken(\"<loc_473>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50743: AddedToken(\"<loc_474>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50744: AddedToken(\"<loc_475>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50745: AddedToken(\"<loc_476>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50746: AddedToken(\"<loc_477>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50747: AddedToken(\"<loc_478>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50748: AddedToken(\"<loc_479>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50749: AddedToken(\"<loc_480>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50750: AddedToken(\"<loc_481>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50751: AddedToken(\"<loc_482>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50752: AddedToken(\"<loc_483>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50753: AddedToken(\"<loc_484>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50754: AddedToken(\"<loc_485>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50755: AddedToken(\"<loc_486>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50756: AddedToken(\"<loc_487>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50757: AddedToken(\"<loc_488>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50758: AddedToken(\"<loc_489>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50759: AddedToken(\"<loc_490>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50760: AddedToken(\"<loc_491>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50761: AddedToken(\"<loc_492>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50762: AddedToken(\"<loc_493>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50763: AddedToken(\"<loc_494>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50764: AddedToken(\"<loc_495>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50765: AddedToken(\"<loc_496>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50766: AddedToken(\"<loc_497>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50767: AddedToken(\"<loc_498>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50768: AddedToken(\"<loc_499>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50769: AddedToken(\"<loc_500>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50770: AddedToken(\"<loc_501>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50771: AddedToken(\"<loc_502>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50772: AddedToken(\"<loc_503>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50773: AddedToken(\"<loc_504>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50774: AddedToken(\"<loc_505>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50775: AddedToken(\"<loc_506>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50776: AddedToken(\"<loc_507>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50777: AddedToken(\"<loc_508>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50778: AddedToken(\"<loc_509>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50779: AddedToken(\"<loc_510>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50780: AddedToken(\"<loc_511>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50781: AddedToken(\"<loc_512>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50782: AddedToken(\"<loc_513>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50783: AddedToken(\"<loc_514>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50784: AddedToken(\"<loc_515>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50785: AddedToken(\"<loc_516>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50786: AddedToken(\"<loc_517>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50787: AddedToken(\"<loc_518>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50788: AddedToken(\"<loc_519>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50789: AddedToken(\"<loc_520>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50790: AddedToken(\"<loc_521>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50791: AddedToken(\"<loc_522>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50792: AddedToken(\"<loc_523>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50793: AddedToken(\"<loc_524>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50794: AddedToken(\"<loc_525>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50795: AddedToken(\"<loc_526>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50796: AddedToken(\"<loc_527>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50797: AddedToken(\"<loc_528>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50798: AddedToken(\"<loc_529>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50799: AddedToken(\"<loc_530>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50800: AddedToken(\"<loc_531>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50801: AddedToken(\"<loc_532>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50802: AddedToken(\"<loc_533>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50803: AddedToken(\"<loc_534>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50804: AddedToken(\"<loc_535>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50805: AddedToken(\"<loc_536>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50806: AddedToken(\"<loc_537>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50807: AddedToken(\"<loc_538>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50808: AddedToken(\"<loc_539>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50809: AddedToken(\"<loc_540>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50810: AddedToken(\"<loc_541>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50811: AddedToken(\"<loc_542>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50812: AddedToken(\"<loc_543>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50813: AddedToken(\"<loc_544>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50814: AddedToken(\"<loc_545>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50815: AddedToken(\"<loc_546>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50816: AddedToken(\"<loc_547>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50817: AddedToken(\"<loc_548>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50818: AddedToken(\"<loc_549>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50819: AddedToken(\"<loc_550>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50820: AddedToken(\"<loc_551>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50821: AddedToken(\"<loc_552>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50822: AddedToken(\"<loc_553>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50823: AddedToken(\"<loc_554>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50824: AddedToken(\"<loc_555>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50825: AddedToken(\"<loc_556>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50826: AddedToken(\"<loc_557>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50827: AddedToken(\"<loc_558>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50828: AddedToken(\"<loc_559>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50829: AddedToken(\"<loc_560>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50830: AddedToken(\"<loc_561>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50831: AddedToken(\"<loc_562>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50832: AddedToken(\"<loc_563>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50833: AddedToken(\"<loc_564>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50834: AddedToken(\"<loc_565>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50835: AddedToken(\"<loc_566>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50836: AddedToken(\"<loc_567>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50837: AddedToken(\"<loc_568>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50838: AddedToken(\"<loc_569>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50839: AddedToken(\"<loc_570>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50840: AddedToken(\"<loc_571>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50841: AddedToken(\"<loc_572>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50842: AddedToken(\"<loc_573>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50843: AddedToken(\"<loc_574>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50844: AddedToken(\"<loc_575>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50845: AddedToken(\"<loc_576>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50846: AddedToken(\"<loc_577>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50847: AddedToken(\"<loc_578>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50848: AddedToken(\"<loc_579>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50849: AddedToken(\"<loc_580>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50850: AddedToken(\"<loc_581>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50851: AddedToken(\"<loc_582>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50852: AddedToken(\"<loc_583>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50853: AddedToken(\"<loc_584>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50854: AddedToken(\"<loc_585>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50855: AddedToken(\"<loc_586>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50856: AddedToken(\"<loc_587>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50857: AddedToken(\"<loc_588>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50858: AddedToken(\"<loc_589>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50859: AddedToken(\"<loc_590>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50860: AddedToken(\"<loc_591>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50861: AddedToken(\"<loc_592>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50862: AddedToken(\"<loc_593>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50863: AddedToken(\"<loc_594>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50864: AddedToken(\"<loc_595>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50865: AddedToken(\"<loc_596>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50866: AddedToken(\"<loc_597>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50867: AddedToken(\"<loc_598>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50868: AddedToken(\"<loc_599>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50869: AddedToken(\"<loc_600>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50870: AddedToken(\"<loc_601>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50871: AddedToken(\"<loc_602>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50872: AddedToken(\"<loc_603>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50873: AddedToken(\"<loc_604>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50874: AddedToken(\"<loc_605>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50875: AddedToken(\"<loc_606>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50876: AddedToken(\"<loc_607>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50877: AddedToken(\"<loc_608>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50878: AddedToken(\"<loc_609>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50879: AddedToken(\"<loc_610>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50880: AddedToken(\"<loc_611>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50881: AddedToken(\"<loc_612>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50882: AddedToken(\"<loc_613>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50883: AddedToken(\"<loc_614>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50884: AddedToken(\"<loc_615>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50885: AddedToken(\"<loc_616>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50886: AddedToken(\"<loc_617>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50887: AddedToken(\"<loc_618>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50888: AddedToken(\"<loc_619>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50889: AddedToken(\"<loc_620>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50890: AddedToken(\"<loc_621>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50891: AddedToken(\"<loc_622>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50892: AddedToken(\"<loc_623>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50893: AddedToken(\"<loc_624>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50894: AddedToken(\"<loc_625>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50895: AddedToken(\"<loc_626>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50896: AddedToken(\"<loc_627>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50897: AddedToken(\"<loc_628>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50898: AddedToken(\"<loc_629>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50899: AddedToken(\"<loc_630>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50900: AddedToken(\"<loc_631>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50901: AddedToken(\"<loc_632>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50902: AddedToken(\"<loc_633>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50903: AddedToken(\"<loc_634>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50904: AddedToken(\"<loc_635>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50905: AddedToken(\"<loc_636>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50906: AddedToken(\"<loc_637>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50907: AddedToken(\"<loc_638>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50908: AddedToken(\"<loc_639>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50909: AddedToken(\"<loc_640>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50910: AddedToken(\"<loc_641>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50911: AddedToken(\"<loc_642>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50912: AddedToken(\"<loc_643>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50913: AddedToken(\"<loc_644>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50914: AddedToken(\"<loc_645>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50915: AddedToken(\"<loc_646>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50916: AddedToken(\"<loc_647>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50917: AddedToken(\"<loc_648>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50918: AddedToken(\"<loc_649>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50919: AddedToken(\"<loc_650>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50920: AddedToken(\"<loc_651>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50921: AddedToken(\"<loc_652>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50922: AddedToken(\"<loc_653>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50923: AddedToken(\"<loc_654>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50924: AddedToken(\"<loc_655>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50925: AddedToken(\"<loc_656>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50926: AddedToken(\"<loc_657>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50927: AddedToken(\"<loc_658>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50928: AddedToken(\"<loc_659>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50929: AddedToken(\"<loc_660>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50930: AddedToken(\"<loc_661>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50931: AddedToken(\"<loc_662>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50932: AddedToken(\"<loc_663>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50933: AddedToken(\"<loc_664>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50934: AddedToken(\"<loc_665>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50935: AddedToken(\"<loc_666>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50936: AddedToken(\"<loc_667>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50937: AddedToken(\"<loc_668>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50938: AddedToken(\"<loc_669>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50939: AddedToken(\"<loc_670>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50940: AddedToken(\"<loc_671>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50941: AddedToken(\"<loc_672>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50942: AddedToken(\"<loc_673>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50943: AddedToken(\"<loc_674>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50944: AddedToken(\"<loc_675>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50945: AddedToken(\"<loc_676>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50946: AddedToken(\"<loc_677>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50947: AddedToken(\"<loc_678>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50948: AddedToken(\"<loc_679>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50949: AddedToken(\"<loc_680>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50950: AddedToken(\"<loc_681>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50951: AddedToken(\"<loc_682>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50952: AddedToken(\"<loc_683>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50953: AddedToken(\"<loc_684>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50954: AddedToken(\"<loc_685>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50955: AddedToken(\"<loc_686>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50956: AddedToken(\"<loc_687>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50957: AddedToken(\"<loc_688>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50958: AddedToken(\"<loc_689>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50959: AddedToken(\"<loc_690>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50960: AddedToken(\"<loc_691>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50961: AddedToken(\"<loc_692>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50962: AddedToken(\"<loc_693>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50963: AddedToken(\"<loc_694>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50964: AddedToken(\"<loc_695>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50965: AddedToken(\"<loc_696>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50966: AddedToken(\"<loc_697>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50967: AddedToken(\"<loc_698>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50968: AddedToken(\"<loc_699>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50969: AddedToken(\"<loc_700>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50970: AddedToken(\"<loc_701>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50971: AddedToken(\"<loc_702>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50972: AddedToken(\"<loc_703>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50973: AddedToken(\"<loc_704>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50974: AddedToken(\"<loc_705>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50975: AddedToken(\"<loc_706>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50976: AddedToken(\"<loc_707>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50977: AddedToken(\"<loc_708>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50978: AddedToken(\"<loc_709>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50979: AddedToken(\"<loc_710>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50980: AddedToken(\"<loc_711>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50981: AddedToken(\"<loc_712>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50982: AddedToken(\"<loc_713>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50983: AddedToken(\"<loc_714>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50984: AddedToken(\"<loc_715>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50985: AddedToken(\"<loc_716>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50986: AddedToken(\"<loc_717>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50987: AddedToken(\"<loc_718>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50988: AddedToken(\"<loc_719>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50989: AddedToken(\"<loc_720>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50990: AddedToken(\"<loc_721>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50991: AddedToken(\"<loc_722>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50992: AddedToken(\"<loc_723>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50993: AddedToken(\"<loc_724>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50994: AddedToken(\"<loc_725>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50995: AddedToken(\"<loc_726>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50996: AddedToken(\"<loc_727>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50997: AddedToken(\"<loc_728>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50998: AddedToken(\"<loc_729>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50999: AddedToken(\"<loc_730>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51000: AddedToken(\"<loc_731>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51001: AddedToken(\"<loc_732>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51002: AddedToken(\"<loc_733>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51003: AddedToken(\"<loc_734>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51004: AddedToken(\"<loc_735>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51005: AddedToken(\"<loc_736>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51006: AddedToken(\"<loc_737>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51007: AddedToken(\"<loc_738>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51008: AddedToken(\"<loc_739>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51009: AddedToken(\"<loc_740>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51010: AddedToken(\"<loc_741>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51011: AddedToken(\"<loc_742>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51012: AddedToken(\"<loc_743>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51013: AddedToken(\"<loc_744>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51014: AddedToken(\"<loc_745>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51015: AddedToken(\"<loc_746>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51016: AddedToken(\"<loc_747>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51017: AddedToken(\"<loc_748>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51018: AddedToken(\"<loc_749>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51019: AddedToken(\"<loc_750>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51020: AddedToken(\"<loc_751>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51021: AddedToken(\"<loc_752>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51022: AddedToken(\"<loc_753>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51023: AddedToken(\"<loc_754>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51024: AddedToken(\"<loc_755>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51025: AddedToken(\"<loc_756>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51026: AddedToken(\"<loc_757>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51027: AddedToken(\"<loc_758>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51028: AddedToken(\"<loc_759>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51029: AddedToken(\"<loc_760>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51030: AddedToken(\"<loc_761>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51031: AddedToken(\"<loc_762>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51032: AddedToken(\"<loc_763>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51033: AddedToken(\"<loc_764>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51034: AddedToken(\"<loc_765>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51035: AddedToken(\"<loc_766>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51036: AddedToken(\"<loc_767>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51037: AddedToken(\"<loc_768>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51038: AddedToken(\"<loc_769>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51039: AddedToken(\"<loc_770>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51040: AddedToken(\"<loc_771>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51041: AddedToken(\"<loc_772>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51042: AddedToken(\"<loc_773>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51043: AddedToken(\"<loc_774>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51044: AddedToken(\"<loc_775>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51045: AddedToken(\"<loc_776>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51046: AddedToken(\"<loc_777>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51047: AddedToken(\"<loc_778>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51048: AddedToken(\"<loc_779>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51049: AddedToken(\"<loc_780>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51050: AddedToken(\"<loc_781>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51051: AddedToken(\"<loc_782>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51052: AddedToken(\"<loc_783>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51053: AddedToken(\"<loc_784>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51054: AddedToken(\"<loc_785>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51055: AddedToken(\"<loc_786>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51056: AddedToken(\"<loc_787>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51057: AddedToken(\"<loc_788>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51058: AddedToken(\"<loc_789>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51059: AddedToken(\"<loc_790>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51060: AddedToken(\"<loc_791>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51061: AddedToken(\"<loc_792>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51062: AddedToken(\"<loc_793>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51063: AddedToken(\"<loc_794>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51064: AddedToken(\"<loc_795>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51065: AddedToken(\"<loc_796>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51066: AddedToken(\"<loc_797>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51067: AddedToken(\"<loc_798>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51068: AddedToken(\"<loc_799>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51069: AddedToken(\"<loc_800>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51070: AddedToken(\"<loc_801>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51071: AddedToken(\"<loc_802>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51072: AddedToken(\"<loc_803>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51073: AddedToken(\"<loc_804>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51074: AddedToken(\"<loc_805>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51075: AddedToken(\"<loc_806>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51076: AddedToken(\"<loc_807>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51077: AddedToken(\"<loc_808>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51078: AddedToken(\"<loc_809>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51079: AddedToken(\"<loc_810>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51080: AddedToken(\"<loc_811>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51081: AddedToken(\"<loc_812>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51082: AddedToken(\"<loc_813>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51083: AddedToken(\"<loc_814>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51084: AddedToken(\"<loc_815>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51085: AddedToken(\"<loc_816>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51086: AddedToken(\"<loc_817>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51087: AddedToken(\"<loc_818>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51088: AddedToken(\"<loc_819>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51089: AddedToken(\"<loc_820>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51090: AddedToken(\"<loc_821>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51091: AddedToken(\"<loc_822>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51092: AddedToken(\"<loc_823>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51093: AddedToken(\"<loc_824>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51094: AddedToken(\"<loc_825>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51095: AddedToken(\"<loc_826>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51096: AddedToken(\"<loc_827>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51097: AddedToken(\"<loc_828>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51098: AddedToken(\"<loc_829>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51099: AddedToken(\"<loc_830>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51100: AddedToken(\"<loc_831>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51101: AddedToken(\"<loc_832>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51102: AddedToken(\"<loc_833>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51103: AddedToken(\"<loc_834>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51104: AddedToken(\"<loc_835>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51105: AddedToken(\"<loc_836>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51106: AddedToken(\"<loc_837>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51107: AddedToken(\"<loc_838>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51108: AddedToken(\"<loc_839>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51109: AddedToken(\"<loc_840>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51110: AddedToken(\"<loc_841>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51111: AddedToken(\"<loc_842>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51112: AddedToken(\"<loc_843>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51113: AddedToken(\"<loc_844>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51114: AddedToken(\"<loc_845>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51115: AddedToken(\"<loc_846>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51116: AddedToken(\"<loc_847>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51117: AddedToken(\"<loc_848>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51118: AddedToken(\"<loc_849>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51119: AddedToken(\"<loc_850>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51120: AddedToken(\"<loc_851>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51121: AddedToken(\"<loc_852>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51122: AddedToken(\"<loc_853>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51123: AddedToken(\"<loc_854>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51124: AddedToken(\"<loc_855>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51125: AddedToken(\"<loc_856>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51126: AddedToken(\"<loc_857>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51127: AddedToken(\"<loc_858>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51128: AddedToken(\"<loc_859>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51129: AddedToken(\"<loc_860>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51130: AddedToken(\"<loc_861>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51131: AddedToken(\"<loc_862>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51132: AddedToken(\"<loc_863>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51133: AddedToken(\"<loc_864>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51134: AddedToken(\"<loc_865>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51135: AddedToken(\"<loc_866>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51136: AddedToken(\"<loc_867>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51137: AddedToken(\"<loc_868>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51138: AddedToken(\"<loc_869>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51139: AddedToken(\"<loc_870>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51140: AddedToken(\"<loc_871>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51141: AddedToken(\"<loc_872>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51142: AddedToken(\"<loc_873>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51143: AddedToken(\"<loc_874>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51144: AddedToken(\"<loc_875>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51145: AddedToken(\"<loc_876>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51146: AddedToken(\"<loc_877>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51147: AddedToken(\"<loc_878>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51148: AddedToken(\"<loc_879>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51149: AddedToken(\"<loc_880>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51150: AddedToken(\"<loc_881>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51151: AddedToken(\"<loc_882>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51152: AddedToken(\"<loc_883>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51153: AddedToken(\"<loc_884>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51154: AddedToken(\"<loc_885>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51155: AddedToken(\"<loc_886>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51156: AddedToken(\"<loc_887>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51157: AddedToken(\"<loc_888>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51158: AddedToken(\"<loc_889>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51159: AddedToken(\"<loc_890>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51160: AddedToken(\"<loc_891>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51161: AddedToken(\"<loc_892>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51162: AddedToken(\"<loc_893>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51163: AddedToken(\"<loc_894>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51164: AddedToken(\"<loc_895>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51165: AddedToken(\"<loc_896>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51166: AddedToken(\"<loc_897>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51167: AddedToken(\"<loc_898>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51168: AddedToken(\"<loc_899>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51169: AddedToken(\"<loc_900>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51170: AddedToken(\"<loc_901>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51171: AddedToken(\"<loc_902>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51172: AddedToken(\"<loc_903>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51173: AddedToken(\"<loc_904>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51174: AddedToken(\"<loc_905>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51175: AddedToken(\"<loc_906>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51176: AddedToken(\"<loc_907>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51177: AddedToken(\"<loc_908>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51178: AddedToken(\"<loc_909>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51179: AddedToken(\"<loc_910>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51180: AddedToken(\"<loc_911>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51181: AddedToken(\"<loc_912>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51182: AddedToken(\"<loc_913>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51183: AddedToken(\"<loc_914>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51184: AddedToken(\"<loc_915>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51185: AddedToken(\"<loc_916>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51186: AddedToken(\"<loc_917>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51187: AddedToken(\"<loc_918>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51188: AddedToken(\"<loc_919>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51189: AddedToken(\"<loc_920>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51190: AddedToken(\"<loc_921>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51191: AddedToken(\"<loc_922>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51192: AddedToken(\"<loc_923>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51193: AddedToken(\"<loc_924>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51194: AddedToken(\"<loc_925>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51195: AddedToken(\"<loc_926>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51196: AddedToken(\"<loc_927>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51197: AddedToken(\"<loc_928>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51198: AddedToken(\"<loc_929>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51199: AddedToken(\"<loc_930>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51200: AddedToken(\"<loc_931>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51201: AddedToken(\"<loc_932>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51202: AddedToken(\"<loc_933>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51203: AddedToken(\"<loc_934>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51204: AddedToken(\"<loc_935>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51205: AddedToken(\"<loc_936>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51206: AddedToken(\"<loc_937>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51207: AddedToken(\"<loc_938>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51208: AddedToken(\"<loc_939>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51209: AddedToken(\"<loc_940>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51210: AddedToken(\"<loc_941>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51211: AddedToken(\"<loc_942>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51212: AddedToken(\"<loc_943>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51213: AddedToken(\"<loc_944>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51214: AddedToken(\"<loc_945>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51215: AddedToken(\"<loc_946>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51216: AddedToken(\"<loc_947>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51217: AddedToken(\"<loc_948>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51218: AddedToken(\"<loc_949>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51219: AddedToken(\"<loc_950>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51220: AddedToken(\"<loc_951>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51221: AddedToken(\"<loc_952>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51222: AddedToken(\"<loc_953>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51223: AddedToken(\"<loc_954>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51224: AddedToken(\"<loc_955>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51225: AddedToken(\"<loc_956>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51226: AddedToken(\"<loc_957>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51227: AddedToken(\"<loc_958>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51228: AddedToken(\"<loc_959>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51229: AddedToken(\"<loc_960>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51230: AddedToken(\"<loc_961>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51231: AddedToken(\"<loc_962>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51232: AddedToken(\"<loc_963>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51233: AddedToken(\"<loc_964>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51234: AddedToken(\"<loc_965>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51235: AddedToken(\"<loc_966>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51236: AddedToken(\"<loc_967>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51237: AddedToken(\"<loc_968>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51238: AddedToken(\"<loc_969>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51239: AddedToken(\"<loc_970>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51240: AddedToken(\"<loc_971>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51241: AddedToken(\"<loc_972>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51242: AddedToken(\"<loc_973>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51243: AddedToken(\"<loc_974>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51244: AddedToken(\"<loc_975>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51245: AddedToken(\"<loc_976>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51246: AddedToken(\"<loc_977>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51247: AddedToken(\"<loc_978>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51248: AddedToken(\"<loc_979>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51249: AddedToken(\"<loc_980>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51250: AddedToken(\"<loc_981>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51251: AddedToken(\"<loc_982>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51252: AddedToken(\"<loc_983>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51253: AddedToken(\"<loc_984>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51254: AddedToken(\"<loc_985>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51255: AddedToken(\"<loc_986>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51256: AddedToken(\"<loc_987>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51257: AddedToken(\"<loc_988>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51258: AddedToken(\"<loc_989>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51259: AddedToken(\"<loc_990>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51260: AddedToken(\"<loc_991>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51261: AddedToken(\"<loc_992>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51262: AddedToken(\"<loc_993>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51263: AddedToken(\"<loc_994>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51264: AddedToken(\"<loc_995>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51265: AddedToken(\"<loc_996>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51266: AddedToken(\"<loc_997>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51267: AddedToken(\"<loc_998>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51268: AddedToken(\"<loc_999>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51269: AddedToken(\"<cap>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51270: AddedToken(\"</cap>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51271: AddedToken(\"<ncap>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51272: AddedToken(\"</ncap>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51273: AddedToken(\"<dcap>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51274: AddedToken(\"</dcap>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51275: AddedToken(\"<grounding>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51276: AddedToken(\"</grounding>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51277: AddedToken(\"<seg>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51278: AddedToken(\"</seg>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51279: AddedToken(\"<sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51280: AddedToken(\"<region_cap>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51281: AddedToken(\"</region_cap>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51282: AddedToken(\"<region_to_desciption>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51283: AddedToken(\"</region_to_desciption>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51284: AddedToken(\"<proposal>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51285: AddedToken(\"</proposal>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51286: AddedToken(\"<poly>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51287: AddedToken(\"</poly>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t51288: AddedToken(\"<and>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"Florence2Processor\"\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3f0e161-01ec-4d8e-9c12-2d4331435edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"florence-2-base.onnx\"\n",
    "# task = \"default\"  # Adjust based on your model type\n",
    "# onnx_config = TasksManager.get_exporter_config_for_model(model, task, exporter=\"onnx\")\n",
    "\n",
    "# model.export(\n",
    "#     onnx_path=onnx_path,\n",
    "#     config=onnx_config,\n",
    "#     device=\"cpu\"  # or \"cuda\" if GPU-based export is supported\n",
    "# )\n",
    "# print(f\"Model exported to {onnx_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cf547fa-3183-4419-8209-2997d2591b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_image = Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "text = \"An example input text for conversion\"\n",
    "inputs = processor(text=text, images=dummy_image, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs.get('attention_mask', torch.ones_like(input_ids, dtype=torch.long))\n",
    "\n",
    "input_ids = inputs['input_ids']\n",
    "pixel_values = inputs['pixel_values']\n",
    "attention_mask = inputs.get('attention_mask', torch.ones_like(input_ids, dtype=torch.long))\n",
    "\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "112a5e36-0def-4c77-a78f-f6fe12ef5ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnjm/.cache/huggingface/modules/transformers_modules/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/modeling_florence2.py:278: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert N == H * W\n",
      "/home/mnjm/.cache/huggingface/modules/transformers_modules/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/modeling_florence2.py:428: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert L == H * W, \"input feature has wrong size\"\n",
      "/home/mnjm/.cache/huggingface/modules/transformers_modules/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/modeling_florence2.py:461: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_r > 0 or pad_b > 0:\n",
      "/home/mnjm/.cache/huggingface/modules/transformers_modules/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/modeling_florence2.py:350: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  q = q * (float(N) ** -0.5)\n",
      "/home/mnjm/.cache/huggingface/modules/transformers_modules/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/modeling_florence2.py:2611: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  h, w = int(num_tokens ** 0.5), int(num_tokens ** 0.5)\n",
      "/home/mnjm/.cache/huggingface/modules/transformers_modules/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/modeling_florence2.py:2612: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert h * w == num_tokens, 'only support square feature maps for now'\n",
      "/home/mnjm/.cache/huggingface/modules/transformers_modules/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/modeling_florence2.py:152: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert len_seq <= self.max_seq_len\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If no `decoder_input_ids` or `decoder_inputs_embeds` are passed, `input_ids` cannot be `None`. Please pass either `input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflorence-2-base.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# input_names=[\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     'input_ids', \u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     'pixel_values', \u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     'attention_mask'\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# ],\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# output_names=['outputs'],\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# dynamic_axes={\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     'input_ids': {0: 'batch_size', 1: 'sequence'},\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     'pixel_values': {0: 'batch_size', 1: 'channels', 2: 'height', 3: 'width'},\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     'attention_mask': {0: 'batch_size', 1: 'sequence'},\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     'outputs': {0: 'batch_size'}\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# }\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/onnx/__init__.py:375\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, kwargs, export_params, verbose, input_names, output_names, opset_version, dynamic_axes, keep_initializers_as_inputs, dynamo, external_data, dynamic_shapes, report, verify, profile, dump_exported_program, artifacts_dir, fallback, training, operator_export_type, do_constant_folding, custom_opsets, export_modules_as_functions, autograd_inlining, **_)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dynamic_shapes:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe exporter only supports dynamic shapes \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthrough parameter dynamic_axes when dynamo=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m     )\n\u001b[0;32m--> 375\u001b[0m \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/onnx/utils.py:502\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, kwargs, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;241m+\u001b[39m (kwargs,)\n\u001b[0;32m--> 502\u001b[0m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/onnx/utils.py:1564\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1562\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1564\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1578\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1579\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1580\u001b[0m )\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/onnx/utils.py:1113\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m   1112\u001b[0m model \u001b[38;5;241m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1113\u001b[0m graph, params, torch_out, module \u001b[38;5;241m=\u001b[39m \u001b[43m_create_jit_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/onnx/utils.py:997\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    992\u001b[0m     graph \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m    993\u001b[0m         graph, flattened_args, param_count_list, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    994\u001b[0m     )\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, params, torch_out, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 997\u001b[0m graph, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_and_get_graph_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    999\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/onnx/utils.py:904\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    902\u001b[0m prev_autocast_cache_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    903\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 904\u001b[0m trace_graph, torch_out, inputs_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    913\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/jit/_trace.py:1500\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   1499\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[0;32m-> 1500\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mONNXTracedModule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/jit/_trace.py:139\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 139\u001b[0m graph, out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_by_tracing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_create_interpreter_name_lookup_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, outs[\u001b[38;5;241m0\u001b[39m], ret_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/jit/_trace.py:130\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    129\u001b[0m     inputs_states\u001b[38;5;241m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 130\u001b[0m outs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrace_inputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    132\u001b[0m     inputs_states[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (inputs_states[\u001b[38;5;241m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/nn/modules/module.py:1726\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1724\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1726\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/modeling_florence2.py:2742\u001b[0m, in \u001b[0;36mFlorence2ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2741\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(inputs_embeds\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m-> 2742\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2745\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2754\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2755\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2756\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2760\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m   2761\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/nn/modules/module.py:1726\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1724\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1726\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/modeling_florence2.py:2141\u001b[0m, in \u001b[0;36mFlorence2LanguageForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2137\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   2138\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   2139\u001b[0m         )\n\u001b[0;32m-> 2141\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2151\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2153\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2154\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2159\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2160\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.py_venv/clip/lib/python3.12/site-packages/torch/nn/modules/module.py:1726\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1724\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1726\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/modeling_florence2.py:1997\u001b[0m, in \u001b[0;36mFlorence2LanguageModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1997\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1998\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf no `decoder_input_ids` or `decoder_inputs_embeds` are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1999\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed, `input_ids` cannot be `None`. Please pass either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2000\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2001\u001b[0m         )\n\u001b[1;32m   2003\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   2004\u001b[0m         input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   2005\u001b[0m     )\n\u001b[1;32m   2007\u001b[0m output_attentions \u001b[38;5;241m=\u001b[39m output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_attentions\n",
      "\u001b[0;31mValueError\u001b[0m: If no `decoder_input_ids` or `decoder_inputs_embeds` are passed, `input_ids` cannot be `None`. Please pass either `input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`."
     ]
    }
   ],
   "source": [
    "output_path = \"florence-2-base.onnx\"\n",
    "torch.onnx.export(\n",
    "        model,\n",
    "        (input_ids, pixel_values, attention_mask),\n",
    "        output_path,\n",
    "        export_params=True,\n",
    "        opset_version=14,\n",
    "        verbose=True,\n",
    "        # input_names=[\n",
    "        #     'input_ids', \n",
    "        #     'pixel_values', \n",
    "        #     'attention_mask'\n",
    "        # ],\n",
    "        # output_names=['outputs'],\n",
    "        # dynamic_axes={\n",
    "        #     'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "        #     'pixel_values': {0: 'batch_size', 1: 'channels', 2: 'height', 3: 'width'},\n",
    "        #     'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
    "        #     'outputs': {0: 'batch_size'}\n",
    "        # }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d893afd-5f2f-48f2-85c2-f61586faec82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
