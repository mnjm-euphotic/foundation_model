{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a48c68c6-814f-408d-a6b6-f6946d6d4bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 10:54:56.966825: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-20 10:54:56.993860: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732080297.025521   19152 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732080297.035344   19152 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 10:54:57.070722: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import onnx\n",
    "import numpy as np\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import onnx2tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f6e227-2dff-4dbf-aaa9-dc1c84e92aca",
   "metadata": {},
   "source": [
    "# Download ViT-B16 from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91bfc93a-c314-4606-9f8b-7a3f7bcce724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ViTForImageClassification(\n",
       "   (vit): ViTModel(\n",
       "     (embeddings): ViTEmbeddings(\n",
       "       (patch_embeddings): ViTPatchEmbeddings(\n",
       "         (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "       )\n",
       "       (dropout): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (encoder): ViTEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x ViTLayer(\n",
       "           (attention): ViTSdpaAttention(\n",
       "             (attention): ViTSdpaSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.0, inplace=False)\n",
       "             )\n",
       "             (output): ViTSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.0, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): ViTIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): ViTOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (dropout): Dropout(p=0.0, inplace=False)\n",
       "           )\n",
       "           (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "           (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "   )\n",
       "   (classifier): Linear(in_features=768, out_features=1000, bias=True)\n",
       " ),\n",
       " ViTImageProcessor {\n",
       "   \"do_normalize\": true,\n",
       "   \"do_rescale\": true,\n",
       "   \"do_resize\": true,\n",
       "   \"image_mean\": [\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5\n",
       "   ],\n",
       "   \"image_processor_type\": \"ViTImageProcessor\",\n",
       "   \"image_std\": [\n",
       "     0.5,\n",
       "     0.5,\n",
       "     0.5\n",
       "   ],\n",
       "   \"resample\": 2,\n",
       "   \"rescale_factor\": 0.00392156862745098,\n",
       "   \"size\": {\n",
       "     \"height\": 224,\n",
       "     \"width\": 224\n",
       "   }\n",
       " })"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"google/vit-base-patch16-224\"\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "model, processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0448bbf6-afbc-43a3-9a66-85b84457b1c1",
   "metadata": {},
   "source": [
    "# Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f36c592-b141-4129-b17d-9871508bed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model verified successfully\n"
     ]
    }
   ],
   "source": [
    "onnx_path = \"vit_b16_huggingface.onnx\"\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "# Get the expected input names\n",
    "input_names = ['pixel_values']\n",
    "output_names = ['logits']\n",
    "\n",
    "# Export to ONNX with opset 14\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    dynamic_axes={\n",
    "        'pixel_values': {0: 'batch_size'},\n",
    "        'logits': {0: 'batch_size'}\n",
    "    },\n",
    "    do_constant_folding=True,\n",
    "    # opset_version=14,  # Updated to opset 14\n",
    "    opset_version=15,  # Updated to opset 14\n",
    "    operator_export_type=torch.onnx.OperatorExportTypes.ONNX_FALLTHROUGH\n",
    ")\n",
    "\n",
    "# Verify ONNX model\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model verified successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680de442-3180-4eef-bdab-a84aeca91b69",
   "metadata": {},
   "source": [
    "# Simplify onnx for tf conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b795169-5802-4344-9f2f-6231e8560db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model simplified successfully\n"
     ]
    }
   ],
   "source": [
    "onnx_simplified_path = \"vit_b16_simplified_huggingface.onnx\"\n",
    "import onnxsim\n",
    "model = onnx.load(onnx_path)\n",
    "model_simp, check = onnxsim.simplify(model)\n",
    "if check:\n",
    "    onnx.save(model_simp, onnx_simplified_path)\n",
    "    print(\"ONNX model simplified successfully\")\n",
    "else:\n",
    "    print(\"ONNX simplification failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d5842-d4f7-4f23-9c1f-631819dc65cf",
   "metadata": {},
   "source": [
    "# Convert ONNX to TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a83adb08-76c3-4af9-936d-b7f7e81e04ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[07mModel optimizing started\u001b[0m ============================================================\n",
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Add        │ 159            │ 159              │\n",
      "│ Concat     │ 3              │ 3                │\n",
      "│ Constant   │ 191            │ 191              │\n",
      "│ Conv       │ 1              │ 1                │\n",
      "│ Div        │ 37             │ 37               │\n",
      "│ Equal      │ 1              │ 1                │\n",
      "│ Erf        │ 12             │ 12               │\n",
      "│ Expand     │ 1              │ 1                │\n",
      "│ Gather     │ 2              │ 2                │\n",
      "│ Gemm       │ 1              │ 1                │\n",
      "│ MatMul     │ 72             │ 72               │\n",
      "│ Mul        │ 73             │ 73               │\n",
      "│ Pow        │ 25             │ 25               │\n",
      "│ ReduceMean │ 50             │ 50               │\n",
      "│ Reshape    │ 49             │ 49               │\n",
      "│ Shape      │ 2              │ 2                │\n",
      "│ Slice      │ 1              │ 1                │\n",
      "│ Softmax    │ 12             │ 12               │\n",
      "│ Split      │ 12             │ 12               │\n",
      "│ Sqrt       │ 25             │ 25               │\n",
      "│ Sub        │ 25             │ 25               │\n",
      "│ Transpose  │ 49             │ 49               │\n",
      "│ Unsqueeze  │ 1              │ 1                │\n",
      "│ Where      │ 1              │ 1                │\n",
      "│ Model Size │ 330.4MiB       │ 330.4MiB         │\n",
      "└────────────┴────────────────┴──────────────────┘\n",
      "\n",
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Add        │ 159            │ 159              │\n",
      "│ Concat     │ 3              │ 3                │\n",
      "│ Constant   │ 191            │ 191              │\n",
      "│ Conv       │ 1              │ 1                │\n",
      "│ Div        │ 37             │ 37               │\n",
      "│ Equal      │ 1              │ 1                │\n",
      "│ Erf        │ 12             │ 12               │\n",
      "│ Expand     │ 1              │ 1                │\n",
      "│ Gather     │ 2              │ 2                │\n",
      "│ Gemm       │ 1              │ 1                │\n",
      "│ MatMul     │ 72             │ 72               │\n",
      "│ Mul        │ 73             │ 73               │\n",
      "│ Pow        │ 25             │ 25               │\n",
      "│ ReduceMean │ 50             │ 50               │\n",
      "│ Reshape    │ 49             │ 49               │\n",
      "│ Shape      │ 2              │ 2                │\n",
      "│ Slice      │ 1              │ 1                │\n",
      "│ Softmax    │ 12             │ 12               │\n",
      "│ Split      │ 12             │ 12               │\n",
      "│ Sqrt       │ 25             │ 25               │\n",
      "│ Sub        │ 25             │ 25               │\n",
      "│ Transpose  │ 49             │ 49               │\n",
      "│ Unsqueeze  │ 1              │ 1                │\n",
      "│ Where      │ 1              │ 1                │\n",
      "│ Model Size │ 330.4MiB       │ 330.4MiB         │\n",
      "└────────────┴────────────────┴──────────────────┘\n",
      "\n",
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Add        │ 159            │ 159              │\n",
      "│ Concat     │ 3              │ 3                │\n",
      "│ Constant   │ 191            │ 191              │\n",
      "│ Conv       │ 1              │ 1                │\n",
      "│ Div        │ 37             │ 37               │\n",
      "│ Equal      │ 1              │ 1                │\n",
      "│ Erf        │ 12             │ 12               │\n",
      "│ Expand     │ 1              │ 1                │\n",
      "│ Gather     │ 2              │ 2                │\n",
      "│ Gemm       │ 1              │ 1                │\n",
      "│ MatMul     │ 72             │ 72               │\n",
      "│ Mul        │ 73             │ 73               │\n",
      "│ Pow        │ 25             │ 25               │\n",
      "│ ReduceMean │ 50             │ 50               │\n",
      "│ Reshape    │ 49             │ 49               │\n",
      "│ Shape      │ 2              │ 2                │\n",
      "│ Slice      │ 1              │ 1                │\n",
      "│ Softmax    │ 12             │ 12               │\n",
      "│ Split      │ 12             │ 12               │\n",
      "│ Sqrt       │ 25             │ 25               │\n",
      "│ Sub        │ 25             │ 25               │\n",
      "│ Transpose  │ 49             │ 49               │\n",
      "│ Unsqueeze  │ 1              │ 1                │\n",
      "│ Where      │ 1              │ 1                │\n",
      "│ Model Size │ 330.4MiB       │ 330.4MiB         │\n",
      "└────────────┴────────────────┴──────────────────┘\n",
      "\n",
      "\u001b[32mModel optimizing complete!\u001b[0m\n",
      "\n",
      "\u001b[07mAutomatic generation of each OP name started\u001b[0m ========================================\n",
      "\u001b[32mAutomatic generation of each OP name complete!\u001b[0m\n",
      "\n",
      "\u001b[07mModel loaded\u001b[0m ========================================================================\n",
      "\n",
      "\u001b[07mModel conversion started\u001b[0m ============================================================\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32minput_op_name\u001b[0m: pixel_values \u001b[32mshape\u001b[0m: ['batch_size', 3, 224, 224] \u001b[32mdtype\u001b[0m: float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 10:56:18.477031: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m2 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/vit/embeddings/Shape\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: pixel_values \u001b[36mshape\u001b[0m: [1, 3, 224, 224] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/embeddings/Shape_output_0 \u001b[36mshape\u001b[0m: [4] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.transpose/transpose:0 \u001b[34mshape\u001b[0m: (1, 224, 224, 3) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape/wa/vit/embeddings/Shape:0 \u001b[34mshape\u001b[0m: (4,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m3 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Conv\u001b[35m onnx_op_name\u001b[0m: wa/vit/embeddings/patch_embeddings/projection/Conv\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: pixel_values \u001b[36mshape\u001b[0m: [1, 3, 224, 224] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: vit.embeddings.patch_embeddings.projection.weight \u001b[36mshape\u001b[0m: [768, 3, 16, 16] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: vit.embeddings.patch_embeddings.projection.bias \u001b[36mshape\u001b[0m: [768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/embeddings/patch_embeddings/projection/Conv_output_0 \u001b[36mshape\u001b[0m: ['batch_size', 768, 14, 14] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: convolution_v2\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.transpose/transpose:0 \u001b[34mshape\u001b[0m: (1, 224, 224, 3) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.weights\u001b[0m: \u001b[34mshape\u001b[0m: (16, 16, 3, 768) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.bias\u001b[0m: \u001b[34mshape\u001b[0m: (768,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.strides\u001b[0m: \u001b[34mval\u001b[0m: [16, 16] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.dilations\u001b[0m: \u001b[34mval\u001b[0m: [1, 1] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.6.padding\u001b[0m: \u001b[34mval\u001b[0m: VALID \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.7.group\u001b[0m: \u001b[34mval\u001b[0m: 1 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add/Add:0 \u001b[34mshape\u001b[0m: (1, 14, 14, 768) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m4 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gather\u001b[35m onnx_op_name\u001b[0m: wa/vit/embeddings/Gather\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/embeddings/Shape_output_0 \u001b[36mshape\u001b[0m: [4] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/vit/embeddings/Constant_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/embeddings/Gather_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: gather_v2\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.params\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape/wa/vit/embeddings/Shape:0 \u001b[34mshape\u001b[0m: (4,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.indices\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add/AddV2:0 \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.axis\u001b[0m: \u001b[34mval\u001b[0m: 0 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.gather/GatherV2:0 \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m5 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Shape\u001b[35m onnx_op_name\u001b[0m: wa/vit/embeddings/patch_embeddings/Shape\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/embeddings/patch_embeddings/projection/Conv_output_0 \u001b[36mshape\u001b[0m: ['batch_size', 768, 14, 14] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/embeddings/patch_embeddings/Shape_output_0 \u001b[36mshape\u001b[0m: [4] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: shape_v2\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add/Add:0 \u001b[34mshape\u001b[0m: (1, 14, 14, 768) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.out_type\u001b[0m: \u001b[34mname\u001b[0m: int64 \u001b[34mshape\u001b[0m: () \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_2/wa/vit/embeddings/patch_embeddings/Shape:0 \u001b[34mshape\u001b[0m: (4,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m6 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Slice\u001b[35m onnx_op_name\u001b[0m: wa/vit/embeddings/patch_embeddings/Slice\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/embeddings/patch_embeddings/Shape_output_0 \u001b[36mshape\u001b[0m: [4] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/vit/embeddings/patch_embeddings/Constant_output_0 \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: wa/vit/embeddings/patch_embeddings/Constant_2_output_0 \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.4\u001b[0m: wa/vit/embeddings/patch_embeddings/Constant_output_0 \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/embeddings/patch_embeddings/Slice_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: strided_slice\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input_\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.shape_2/wa/vit/embeddings/patch_embeddings/Shape:0 \u001b[34mshape\u001b[0m: (4,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.begin\u001b[0m: \u001b[34mshape\u001b[0m: (1,) \u001b[34mdtype\u001b[0m: int64 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.end\u001b[0m: \u001b[34mshape\u001b[0m: (1,) \u001b[34mdtype\u001b[0m: int64 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.strides\u001b[0m: \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.strided_slice/StridedSlice:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m7 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Unsqueeze\u001b[35m onnx_op_name\u001b[0m: wa/vit/embeddings/Unsqueeze\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/embeddings/Gather_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/vit/embeddings/patch_embeddings/Constant_output_0 \u001b[36mshape\u001b[0m: (1,) \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/embeddings/Unsqueeze_output_0 \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: reshape\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.gather/GatherV2:0 \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.shape\u001b[0m: \u001b[34mval\u001b[0m: [1] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape/Reshape:0 \u001b[34mshape\u001b[0m: (1,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m8 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Concat\u001b[35m onnx_op_name\u001b[0m: wa/vit/embeddings/patch_embeddings/Concat\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/embeddings/patch_embeddings/Slice_output_0 \u001b[36mshape\u001b[0m: [2] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/vit/embeddings/patch_embeddings/Constant_3_output_0 \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/embeddings/patch_embeddings/Concat_output_0 \u001b[36mshape\u001b[0m: [3] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: concat\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input0\u001b[0m: \u001b[34mname\u001b[0m: tf.strided_slice/StridedSlice:0 \u001b[34mshape\u001b[0m: (2,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input1\u001b[0m: \u001b[34mshape\u001b[0m: (1,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.axis\u001b[0m: \u001b[34mval\u001b[0m: 0 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.concat/concat:0 \u001b[34mshape\u001b[0m: (3,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m9 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Concat\u001b[35m onnx_op_name\u001b[0m: wa/vit/embeddings/Concat\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/embeddings/Unsqueeze_output_0 \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/vit/embeddings/patch_embeddings/Constant_3_output_0 \u001b[36mshape\u001b[0m: (1,) \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: wa/vit/embeddings/patch_embeddings/Constant_3_output_0 \u001b[36mshape\u001b[0m: (1,) \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/embeddings/Concat_output_0 \u001b[36mshape\u001b[0m: [3] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: concat\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input0\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape/Reshape:0 \u001b[34mshape\u001b[0m: (1,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input1\u001b[0m: \u001b[34mshape\u001b[0m: (1,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.input2\u001b[0m: \u001b[34mshape\u001b[0m: (1,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.axis\u001b[0m: \u001b[34mval\u001b[0m: 0 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.concat_1/concat:0 \u001b[34mshape\u001b[0m: (3,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m10 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Reshape\u001b[35m onnx_op_name\u001b[0m: wa/vit/embeddings/patch_embeddings/Reshape\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/embeddings/patch_embeddings/projection/Conv_output_0 \u001b[36mshape\u001b[0m: ['batch_size', 768, 14, 14] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/vit/embeddings/patch_embeddings/Concat_output_0 \u001b[36mshape\u001b[0m: [3] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/embeddings/patch_embeddings/Reshape_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 'unk__1', 'unk__2'] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.transpose_3/transpose:0 \u001b[34mshape\u001b[0m: (1, 768, 14, 14) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.shape\u001b[0m: \u001b[34mname\u001b[0m: tf.concat/concat:0 \u001b[34mshape\u001b[0m: (3,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape_1/Reshape:0 \u001b[34mshape\u001b[0m: (None, None, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m11 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Equal\u001b[35m onnx_op_name\u001b[0m: wa/vit/embeddings/Equal\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/embeddings/Concat_output_0 \u001b[36mshape\u001b[0m: [3] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/vit/embeddings/Mul_output_0 \u001b[36mshape\u001b[0m: [3] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/embeddings/Equal_output_0 \u001b[36mshape\u001b[0m: [3] \u001b[36mdtype\u001b[0m: bool\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: equal\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.concat_1/concat:0 \u001b[34mshape\u001b[0m: (3,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (3,) \u001b[34mdtype\u001b[0m: int64 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.equal/Equal:0 \u001b[34mshape\u001b[0m: (3,) \u001b[34mdtype\u001b[0m: <dtype: 'bool'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m12 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Transpose\u001b[35m onnx_op_name\u001b[0m: wa/vit/embeddings/patch_embeddings/Transpose\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/embeddings/patch_embeddings/Reshape_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 'unk__1', 'unk__2'] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/embeddings/patch_embeddings/Transpose_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 'unk__2', 'unk__1'] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: transpose_v2\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.a\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape_1/Reshape:0 \u001b[34mshape\u001b[0m: (None, None, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.perm\u001b[0m: \u001b[34mval\u001b[0m: [0, 1, 2] \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape_1/Reshape:0 \u001b[34mshape\u001b[0m: (None, None, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m13 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Where\u001b[35m onnx_op_name\u001b[0m: wa/vit/embeddings/Where\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/embeddings/Equal_output_0 \u001b[36mshape\u001b[0m: [3] \u001b[36mdtype\u001b[0m: bool\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/vit/embeddings/ConstantOfShape_output_0 \u001b[36mshape\u001b[0m: [3] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: wa/vit/embeddings/Concat_output_0 \u001b[36mshape\u001b[0m: [3] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/embeddings/Where_output_0 \u001b[36mshape\u001b[0m: [3] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: where_v2\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.condition\u001b[0m: \u001b[34mname\u001b[0m: tf.math.equal/Equal:0 \u001b[34mshape\u001b[0m: (3,) \u001b[34mdtype\u001b[0m: <dtype: 'bool'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.x\u001b[0m: \u001b[34mshape\u001b[0m: (3,) \u001b[34mdtype\u001b[0m: int64 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.y\u001b[0m: \u001b[34mname\u001b[0m: tf.concat_1/concat:0 \u001b[34mshape\u001b[0m: (3,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.where/SelectV2:0 \u001b[34mshape\u001b[0m: (3,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m14 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Expand\u001b[35m onnx_op_name\u001b[0m: wa/vit/embeddings/Expand\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: vit.embeddings.cls_token \u001b[36mshape\u001b[0m: [1, 1, 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/vit/embeddings/Where_output_0 \u001b[36mshape\u001b[0m: [3] \u001b[36mdtype\u001b[0m: int64\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/embeddings/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 'unk__5', 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: Expand\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input_tensor\u001b[0m: \u001b[34mshape\u001b[0m: (1, 768, 1) \u001b[34mdtype\u001b[0m: float32 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor_shape\u001b[0m: \u001b[34mname\u001b[0m: tf.where/SelectV2:0 \u001b[34mshape\u001b[0m: (3,) \u001b[34mdtype\u001b[0m: <dtype: 'int64'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_1/Mul:0 \u001b[34mshape\u001b[0m: (None, 768, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m15 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Concat\u001b[35m onnx_op_name\u001b[0m: wa/vit/embeddings/Concat_1\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/embeddings/Expand_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 'unk__5', 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/vit/embeddings/patch_embeddings/Transpose_output_0 \u001b[36mshape\u001b[0m: ['unk__0', 'unk__2', 'unk__1'] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/embeddings/Concat_1_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 'unk__6', 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: concat\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input0\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_1/Mul:0 \u001b[34mshape\u001b[0m: (None, 768, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input1\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape_1/Reshape:0 \u001b[34mshape\u001b[0m: (None, None, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.axis\u001b[0m: \u001b[34mval\u001b[0m: 2 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.concat_39/concat:0 \u001b[34mshape\u001b[0m: (None, 768, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m16 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Add\u001b[35m onnx_op_name\u001b[0m: wa/vit/embeddings/Add\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/embeddings/Concat_1_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 'unk__6', 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: vit.embeddings.position_embeddings \u001b[36mshape\u001b[0m: [1, 197, 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/embeddings/Add_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: add\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.concat_39/concat:0 \u001b[34mshape\u001b[0m: (None, 768, None) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (1, 768, 197) \u001b[34mdtype\u001b[0m: float32 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_1/Add:0 \u001b[34mshape\u001b[0m: (None, 768, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m17 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: ReduceMean\u001b[35m onnx_op_name\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/ReduceMean\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/embeddings/Add_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/ReduceMean_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 1] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: reduce_mean\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.axis0\u001b[0m: \u001b[34mval\u001b[0m: 1 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_1/Add:0 \u001b[34mshape\u001b[0m: (None, 768, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.keepdims\u001b[0m: \u001b[34mval\u001b[0m: True \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.reduce_mean/Mean:0 \u001b[34mshape\u001b[0m: (None, 1, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m18 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sub\u001b[35m onnx_op_name\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Sub\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/embeddings/Add_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/ReduceMean_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 1] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Sub_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: subtract\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_1/Add:0 \u001b[34mshape\u001b[0m: (None, 768, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.reduce_mean/Mean:0 \u001b[34mshape\u001b[0m: (None, 1, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract/Sub:0 \u001b[34mshape\u001b[0m: (None, 768, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m19 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Pow\u001b[35m onnx_op_name\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Pow\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Sub_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Constant_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Pow_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: pow\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract/Sub:0 \u001b[34mshape\u001b[0m: (None, 768, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: float32 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_3/Mul:0 \u001b[34mshape\u001b[0m: (None, 768, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m20 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: ReduceMean\u001b[35m onnx_op_name\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/ReduceMean_1\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Pow_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/ReduceMean_1_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 1] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: reduce_mean\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.axis0\u001b[0m: \u001b[34mval\u001b[0m: 1 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.input_tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_3/Mul:0 \u001b[34mshape\u001b[0m: (None, 768, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.keepdims\u001b[0m: \u001b[34mval\u001b[0m: True \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.reduce_mean_1/Mean:0 \u001b[34mshape\u001b[0m: (None, 1, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m21 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Add\u001b[35m onnx_op_name\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Add\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/ReduceMean_1_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 1] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Constant_1_output_0 \u001b[36mshape\u001b[0m: [] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Add_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 1] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: add\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.reduce_mean_1/Mean:0 \u001b[34mshape\u001b[0m: (None, 1, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: () \u001b[34mdtype\u001b[0m: float32 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_3/Add:0 \u001b[34mshape\u001b[0m: (None, 1, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m22 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sqrt\u001b[35m onnx_op_name\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Sqrt\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Add_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 1] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Sqrt_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 1] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: sqrt\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_3/Add:0 \u001b[34mshape\u001b[0m: (None, 1, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.sqrt/Sqrt:0 \u001b[34mshape\u001b[0m: (None, 1, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m23 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Div\u001b[35m onnx_op_name\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Div\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Sub_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Sqrt_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 1] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Div_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 768] \u001b[36mdtype\u001b[0m: float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 10:56:22.904661: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [1,768,1] vs. shape[1] = [1,14,10752]\n",
      "2024-11-20 10:56:22.977097: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [1,768,1] vs. shape[1] = [1,14,10752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: divide\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.subtract/Sub:0 \u001b[34mshape\u001b[0m: (None, 768, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mname\u001b[0m: tf.math.sqrt/Sqrt:0 \u001b[34mshape\u001b[0m: (None, 1, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.divide/truediv:0 \u001b[34mshape\u001b[0m: (None, 768, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m24 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Mul\u001b[35m onnx_op_name\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Mul\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Div_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: vit.encoder.layer.0.layernorm_before.weight \u001b[36mshape\u001b[0m: [768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Mul_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: multiply\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.divide/truediv:0 \u001b[34mshape\u001b[0m: (None, 768, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (1, 768, 1) \u001b[34mdtype\u001b[0m: float32 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_5/Mul:0 \u001b[34mshape\u001b[0m: (None, 768, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m25 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Add\u001b[35m onnx_op_name\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Add_1\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Mul_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: vit.encoder.layer.0.layernorm_before.bias \u001b[36mshape\u001b[0m: [768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: add\n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.math.multiply_5/Mul:0 \u001b[34mshape\u001b[0m: (None, 768, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (1, 768, 1) \u001b[34mdtype\u001b[0m: float32 \n",
      "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_4/Add:0 \u001b[34mshape\u001b[0m: (None, 768, 197) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
      "\n",
      "\u001b[32mINFO:\u001b[0m \u001b[32m26 / 615\u001b[0m\n",
      "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: MatMul\u001b[35m onnx_op_name\u001b[0m: sng_MatMul_0\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: wa/vit/encoder/layer.0/layernorm_before/Add_1_output_0 \u001b[36mshape\u001b[0m: ['unk__4', 197, 768] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: _v_2564 \u001b[36mshape\u001b[0m: [768, 2304] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: _v_2565 \u001b[36mshape\u001b[0m: ['unk__4', 197, 2304] \u001b[36mdtype\u001b[0m: float32\n",
      "\u001b[31mERROR:\u001b[0m The trace log is below.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mnjm/.py_venv/clip/lib/python3.12/site-packages/onnx2tf/utils/common_functions.py\", line 312, in print_wrapper_func\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mnjm/.py_venv/clip/lib/python3.12/site-packages/onnx2tf/utils/common_functions.py\", line 385, in inverted_operation_enable_disable_wrapper_func\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mnjm/.py_venv/clip/lib/python3.12/site-packages/onnx2tf/utils/common_functions.py\", line 55, in get_replacement_parameter_wrapper_func\n",
      "    func(*args, **kwargs)\n",
      "  File \"/home/mnjm/.py_venv/clip/lib/python3.12/site-packages/onnx2tf/ops/MatMul.py\", line 302, in make_node\n",
      "    define_matmul(\n",
      "  File \"/home/mnjm/.py_venv/clip/lib/python3.12/site-packages/onnx2tf/ops/MatMul.py\", line 148, in define_matmul\n",
      "    tf.matmul(\n",
      "  File \"/home/mnjm/.py_venv/clip/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py\", line 142, in wrapper\n",
      "    return op(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mnjm/.py_venv/clip/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/mnjm/.py_venv/clip/lib/python3.12/site-packages/tf_keras/src/layers/core/tf_op_layer.py\", line 119, in handle\n",
      "    return TFOpLambda(op)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mnjm/.py_venv/clip/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py\", line 72, in error_handler\n",
      "    del filtered_tb\n",
      "        ^^^^^^^^^^^\n",
      "ValueError: Exception encountered when calling layer \"tf.linalg.matmul\" (type TFOpLambda).\n",
      "\n",
      "Dimensions must be equal, but are 197 and 768 for '{{node tf.linalg.matmul/MatMul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false, grad_x=false, grad_y=false](Placeholder, tf.linalg.matmul/MatMul/b)' with input shapes: [?,768,197], [768,2304].\n",
      "\n",
      "Call arguments received by layer \"tf.linalg.matmul\" (type TFOpLambda):\n",
      "  • a=tf.Tensor(shape=(None, 768, 197), dtype=float32)\n",
      "  • b=tf.Tensor(shape=(768, 2304), dtype=float32)\n",
      "  • transpose_a=False\n",
      "  • transpose_b=False\n",
      "  • adjoint_a=False\n",
      "  • adjoint_b=False\n",
      "  • a_is_sparse=False\n",
      "  • b_is_sparse=False\n",
      "  • output_type=tf.float32\n",
      "  • grad_a=False\n",
      "  • grad_b=False\n",
      "  • name='sng_MatMul_0'\n",
      "\n",
      "\u001b[31mERROR:\u001b[0m input_onnx_file_path: vit_b16_simplified_huggingface.onnx\n",
      "\u001b[31mERROR:\u001b[0m onnx_op_name: sng_MatMul_0\n",
      "\u001b[31mERROR:\u001b[0m Read this and deal with it. https://github.com/PINTO0309/onnx2tf#parameter-replacement\n",
      "\u001b[31mERROR:\u001b[0m Alternatively, if the input OP has a dynamic dimension, use the -b or -ois option to rewrite it to a static shape and try again.\n",
      "\u001b[31mERROR:\u001b[0m If the input OP of ONNX before conversion is NHWC or an irregular channel arrangement other than NCHW, use the -kt or -kat option.\n",
      "\u001b[31mERROR:\u001b[0m Also, for models that include NonMaxSuppression in the post-processing, try the -onwdt option.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "tf_path = \"vit_b16_huggingface\"\n",
    "# onnx2tf.convert(\n",
    "#         input_onnx_file_path=onnx_path,\n",
    "#         output_folder_path=tf_path,\n",
    "#         output_signaturedefs=True,\n",
    "# )\n",
    "import json\n",
    "\n",
    "config = {\n",
    "        \"input_shapes\": {\n",
    "            \"pixel_values\": [1, 3, 224, 224]\n",
    "        },\n",
    "        \"output_shapes\": {\n",
    "            \"logits\": [1, 1000]\n",
    "        }\n",
    "    }\n",
    "\n",
    "shape_config_json = 'shape_config.json'\n",
    "\n",
    "with open(shape_config_json, 'w') as f:\n",
    "        json.dump(config, f)\n",
    " \n",
    "onnx2tf.convert(\n",
    "        input_onnx_file_path=onnx_simplified_path,\n",
    "        output_folder_path=tf_path,\n",
    "        output_signaturedefs=True,\n",
    "        # Shape inference options\n",
    "        batch_size=1,\n",
    "        keep_ncw_or_nchw_or_ncdhw_input_names=['pixel_values'],\n",
    "        # Additional options to handle dimension issues\n",
    "        # custom_output_shape_dict=shape_config_json,\n",
    "        # skip_unknown_shape=True,\n",
    "        # keep_input_tensor_shapes=True,\n",
    "        # Optimization options\n",
    "        # preserve_custom_attributes=True,\n",
    "        # parallel_processing=True,\n",
    "        # optimization_for_gpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df879af6-af7b-49ac-bac9-aa36b999d8df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
