{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de853f9-06b8-415b-8a44-45671874e08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 17:19:24.238598: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-22 17:19:24.249135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732276164.262370   16768 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732276164.266073   16768 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-22 17:19:24.278601: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d544bf71-1050-45d7-af3e-560159450f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5979425af192494c9891fef48dc695c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6560e77e9b47a6909cc11c2a0193cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/813M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f08071d83e4270b8ac670ec1b3276b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/368 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfc9c8891884854b19f0143c9575fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/711 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fa6f504f5a4742a752640be93805f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc88199ce084a77a2eb3e775e556a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/409 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429030fda27644938c23974bb95fb046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.9% that image 0 is 'a photo of 2 cats'\n",
      "0.0% that image 0 is 'a photo of 2 dogs'\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/siglip-base-patch16-224\"\n",
    "# model_name = \"google/siglip-so400m-patch14-224\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "texts = [\"a photo of 2 cats\", \"a photo of 2 dogs\"]\n",
    "inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = torch.sigmoid(logits_per_image) # these are the probabilities\n",
    "print(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")\n",
    "print(f\"{probs[0][1]:.1%} that image 0 is '{texts[1]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b456f01d-05f2-4b2b-ab21-c16ba28472ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipModel(\n",
       "  (text_model): SiglipTextTransformer(\n",
       "    (embeddings): SiglipTextEmbeddings(\n",
       "      (token_embedding): Embedding(32000, 768)\n",
       "      (position_embedding): Embedding(64, 768)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (vision_model): SiglipVisionTransformer(\n",
       "    (embeddings): SiglipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "      (position_embedding): Embedding(196, 768)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): SiglipMultiheadAttentionPoolingHead(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): SiglipMLP(\n",
       "        (activation_fn): PytorchGELUTanh()\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b0c80d-5b3a-46a9-8e2f-6a2a04284648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipVisionTransformer(\n",
       "  (embeddings): SiglipVisionEmbeddings(\n",
       "    (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "    (position_embedding): Embedding(196, 768)\n",
       "  )\n",
       "  (encoder): SiglipEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x SiglipEncoderLayer(\n",
       "        (self_attn): SiglipSdpaAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): SiglipMLP(\n",
       "          (activation_fn): PytorchGELUTanh()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): SiglipMultiheadAttentionPoolingHead(\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): SiglipMLP(\n",
       "      (activation_fn): PytorchGELUTanh()\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_model = model.vision_model\n",
    "vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7543aa6-b8cb-49e0-9b0b-dee1510f58ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.6195,  0.2554, -1.0134,  ..., -0.5341,  0.4381, -1.6009],\n",
       "         [ 0.4081,  0.3242, -0.4967,  ..., -1.4876, -1.8570, -1.9226],\n",
       "         [-0.7247,  1.4924, -1.0048,  ..., -2.0564,  0.2459,  0.4242],\n",
       "         ...,\n",
       "         [ 0.7666, -0.1956, -1.1861,  ..., -0.5558,  1.5912, -2.1294],\n",
       "         [ 0.0407,  0.3529, -1.6780,  ...,  0.6625, -0.1729, -1.9680],\n",
       "         [ 1.0815, -1.5693, -0.5918,  ..., -0.2207,  1.1262, -0.3552]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-3.8798e-02, -1.8350e-01,  2.8249e-01, -1.8911e-01, -1.4829e-01,\n",
       "          2.2499e-02, -5.2317e-01, -4.5164e-01, -2.3904e-01,  1.8428e-01,\n",
       "         -2.7591e-01, -2.3815e-01,  6.1551e-01, -6.1196e-01, -5.0202e-02,\n",
       "         -4.5417e-01,  1.9159e-01,  3.3827e-02,  2.4865e-01, -1.0768e-01,\n",
       "          3.9088e-01,  1.5782e-01, -1.3214e-01, -2.2701e-01, -3.8112e-01,\n",
       "         -2.1870e-01,  1.0756e-01,  1.8085e-01,  2.8722e-01,  4.2217e-02,\n",
       "         -3.8018e-01,  2.7789e-01, -9.3007e-02,  3.5485e-01, -1.7014e-02,\n",
       "         -3.6841e-01,  3.0010e-01,  2.5339e-01,  2.7617e-01,  5.3406e-01,\n",
       "         -4.3003e-01, -3.0062e-02, -3.5411e-01, -1.0978e-01, -1.2041e-01,\n",
       "         -3.3377e-01,  1.7753e-01, -4.1171e-01,  3.8185e-01,  3.3664e-01,\n",
       "          6.1548e-01, -7.4761e-01, -1.1251e-01, -4.1430e-01, -8.1422e-01,\n",
       "         -7.7736e-01,  4.6604e-01, -1.1180e-02,  3.8256e-01,  3.7888e-02,\n",
       "          1.9855e-01,  1.0759e+00, -3.0326e-01,  2.9964e-01, -2.4098e-01,\n",
       "          7.2181e-03,  5.3694e-01, -1.7772e-01,  1.2700e-01,  2.2373e-01,\n",
       "          4.6225e-01,  1.0890e-01, -5.5541e-01, -2.1690e-01,  2.2260e-01,\n",
       "          6.9314e-01,  3.4462e-01,  1.9680e-01,  7.7567e+00, -3.5164e-01,\n",
       "          1.6821e-01, -4.4289e-01,  1.8750e-01,  5.8489e-01, -1.5385e-01,\n",
       "          6.4885e-01, -5.0591e-02, -5.2116e-01,  1.3383e-01,  3.4001e-01,\n",
       "         -3.7253e-01,  2.9255e-01,  4.0244e-01, -9.0789e-02,  1.4385e+00,\n",
       "         -1.7530e-01, -3.6402e-01, -1.3694e-01,  1.7069e-01, -1.0770e-01,\n",
       "         -1.1706e-01, -1.1912e-01, -6.1639e-02,  3.2108e-01,  6.7423e-01,\n",
       "         -1.6551e-01, -7.3284e-01,  3.8832e-01,  6.9717e-01,  4.3305e-01,\n",
       "          1.4736e-01,  2.0364e-01,  1.1807e-01,  2.0398e-01, -2.8243e-01,\n",
       "         -5.0945e-01, -5.0339e-01,  1.4449e-01, -3.8700e-01,  4.3645e-01,\n",
       "         -4.4569e-01,  7.1736e-01,  3.8887e-01, -4.4132e-01,  2.0133e-01,\n",
       "          5.3665e-01, -5.2617e-01, -2.2236e-01,  1.4195e+00,  2.5422e-01,\n",
       "         -3.0031e-02,  4.1335e-01,  3.2150e-01,  1.0533e-01,  8.7498e-02,\n",
       "          3.8520e-01, -1.5939e-01, -1.0751e-01,  5.6109e-01,  7.7111e-02,\n",
       "         -1.7306e-01,  9.3912e-01,  1.7239e-01, -4.9738e-01,  1.3830e+00,\n",
       "          2.7507e-01,  1.1779e-01, -1.3626e-01,  1.5606e-01, -2.5913e-01,\n",
       "          1.3676e-01,  4.4998e-02,  7.4784e-02,  6.0391e+00,  4.8252e-01,\n",
       "         -3.0841e-01, -1.5978e-01,  3.2880e-01,  1.4889e-01,  1.8390e-02,\n",
       "          1.3482e-01,  5.8585e-04,  6.2282e-01, -4.2619e-01, -3.7043e-01,\n",
       "         -4.4541e-01, -1.9970e-02,  2.9812e-01,  1.5860e-01,  5.6923e-01,\n",
       "         -4.6444e-01, -3.4241e-01, -3.7522e-01,  2.6843e-01, -6.4772e-01,\n",
       "         -3.5153e-01, -5.5937e-01, -2.2753e-01, -2.8037e-01, -4.4089e-01,\n",
       "         -1.8891e-01,  3.6766e-01,  8.0500e-02, -1.3671e-01, -1.5959e-01,\n",
       "          1.5644e-01,  3.3401e-01,  1.5502e-01, -8.2292e-02, -1.1646e-01,\n",
       "          4.4504e-02,  4.4192e-01,  3.1354e-01,  4.6525e-02,  9.3989e-02,\n",
       "          1.9937e-01, -6.3164e-01, -2.2035e-02,  9.6476e-02,  5.9480e-03,\n",
       "         -1.6493e-02,  5.2960e-01, -3.8327e-02, -3.7814e-01,  2.0983e-01,\n",
       "         -5.2462e-02,  1.1475e-01,  4.8242e-01, -2.3674e-01,  1.9488e-01,\n",
       "         -3.8101e-01,  1.2050e+00,  1.3483e-01, -2.1446e-01, -2.8776e-01,\n",
       "         -1.5662e-01, -1.2682e+00,  1.9355e-01, -3.1476e-01,  5.8866e-01,\n",
       "          2.7385e-01, -1.3419e-02, -1.9585e-01,  2.4644e-01, -8.1444e-02,\n",
       "          8.9801e-01, -6.6776e-02,  2.6641e-02, -7.3287e-01, -2.2729e-01,\n",
       "         -3.6584e-02,  1.8611e-01,  6.8588e-02,  2.4512e-01,  7.2863e-02,\n",
       "         -5.0763e-02,  5.4863e-01,  8.9247e-02,  1.6942e-02,  2.5494e-01,\n",
       "          8.4610e-02, -3.2060e-01,  3.9550e-01, -5.1668e-02, -7.4701e-02,\n",
       "          2.2463e-01, -4.4468e-01, -3.8754e-01,  4.2666e-01,  7.9316e-02,\n",
       "          3.0099e-02,  1.5499e-01, -1.1032e-01, -4.3231e-01, -3.3196e-03,\n",
       "          3.8018e-02,  5.6288e-01, -3.7300e-01,  9.2620e-02, -2.7221e-01,\n",
       "         -1.1243e+00, -2.3020e-01, -2.5849e-01,  2.4897e-01, -1.1072e-01,\n",
       "         -1.6254e-01,  5.2293e-01, -9.7578e-02, -4.3857e-01,  2.9761e-02,\n",
       "         -1.0376e+00,  2.9283e-01,  5.9602e-01, -3.3658e-01,  6.8747e-02,\n",
       "          4.2074e-01, -1.1270e+00, -4.4266e-01, -8.3994e-02,  2.2294e-01,\n",
       "          1.5015e-01,  5.9566e-02,  5.3695e-04, -6.0869e-01, -5.4395e-02,\n",
       "         -2.1229e-01,  4.5055e-01,  3.4091e-03,  2.9084e-01, -1.3131e-01,\n",
       "         -2.5283e-01, -1.4675e-01,  1.4711e-01, -5.0316e-01, -2.8478e-01,\n",
       "          4.2844e-04, -3.9053e-01, -7.4233e-01,  3.2370e-01,  5.0361e-02,\n",
       "          2.8966e-02,  4.8133e+00, -1.7756e-01,  1.5639e-01,  8.6137e-02,\n",
       "         -7.2196e-02, -1.6765e-01, -5.5031e-02,  8.6711e-01, -2.7579e-01,\n",
       "         -1.3619e-01,  1.8169e-01, -2.3943e-01,  5.4342e-01, -4.3659e-01,\n",
       "         -8.9191e-01,  3.2690e-01,  5.4661e-01, -2.1280e-01,  3.8611e-01,\n",
       "          1.1474e-01, -3.2123e-01,  1.2525e-02, -9.5651e-04, -4.4642e-01,\n",
       "         -3.2082e-02, -1.6493e-01, -2.2730e-02,  2.4661e-01, -1.2859e-01,\n",
       "          3.9566e-01, -1.1290e-01, -3.5423e-01, -1.0602e-01, -1.2966e-01,\n",
       "         -5.5142e-01, -1.0290e-02, -5.2429e-01,  5.1076e-01,  3.5495e-02,\n",
       "          4.6786e-01,  4.5783e-01, -5.1583e-03,  1.1778e-01, -1.0065e+00,\n",
       "         -5.3910e-01, -3.4594e-02,  5.1039e-01, -1.0467e+00,  1.3960e+00,\n",
       "         -2.5710e-01,  6.1799e-02, -3.1817e-01, -1.9341e-01, -9.4808e-02,\n",
       "         -1.5839e-01, -5.7108e-02, -1.8313e-01,  1.2006e-02,  7.7138e-01,\n",
       "          1.4512e-02, -1.5471e-01,  8.8863e-02, -3.9702e-01,  7.3534e-02,\n",
       "         -9.9869e-02,  6.7350e-02, -1.3415e-01,  4.6356e-01, -2.0621e-01,\n",
       "         -2.3382e-01,  5.4996e-02,  2.2400e-01,  3.3487e-01,  3.9026e-01,\n",
       "          3.2279e-01, -3.6341e-01,  7.8475e-01,  2.0552e-01,  5.7590e-01,\n",
       "         -4.1559e-01,  2.3739e-01, -1.0576e+00, -3.5290e-01,  1.8205e-01,\n",
       "         -2.0206e-01, -1.8109e-01,  1.8115e-01, -3.9395e-02,  1.0870e-01,\n",
       "         -1.1466e-03, -8.6239e-02,  4.8857e-01,  1.3734e-01,  1.3278e+00,\n",
       "         -4.9259e-01, -5.2622e-01, -6.1902e-01, -4.1901e-01, -3.4954e-01,\n",
       "          8.4085e-02,  6.2568e-01, -2.1180e-02,  9.9322e-01,  6.3342e-01,\n",
       "          7.9368e-01,  3.6033e-01, -4.6725e-01,  8.0838e-02,  3.3126e-02,\n",
       "         -6.9643e-01,  2.8277e-01, -3.4238e-01, -2.7217e-01,  1.1815e-01,\n",
       "         -3.2830e-01,  2.5594e-01,  1.2449e-02, -1.9880e-01,  2.6485e-01,\n",
       "          8.3578e-02, -9.3779e-02, -3.0063e-01,  1.5401e-01,  2.9332e-01,\n",
       "         -5.6201e-01, -3.9983e-02, -3.5243e-01, -1.8871e+00,  5.0952e-01,\n",
       "          1.4693e-01,  5.7074e-01, -1.8445e-01, -2.4221e-01,  1.9339e-01,\n",
       "         -4.9219e-02, -4.4118e-01,  5.3210e-01, -1.3067e-01,  2.1117e-02,\n",
       "          4.5734e-01,  5.4826e-01,  1.4490e-01,  5.6832e-01,  4.3410e-01,\n",
       "          6.1284e-02,  9.6429e-02, -2.3788e-02, -2.5130e-01, -9.8456e-01,\n",
       "          5.2272e-02, -1.5181e-01,  1.8735e-01,  3.7646e-01,  5.6203e-01,\n",
       "          1.2426e-01,  1.3627e-01, -2.6170e-01,  1.1586e-01,  1.1642e-01,\n",
       "          1.5978e-01, -4.3120e-01, -4.6027e-01,  4.7314e-01, -3.8025e-01,\n",
       "         -3.2507e-01,  7.1660e-01,  1.0930e-01,  9.9867e-02, -1.4246e-02,\n",
       "         -4.5187e-02,  3.4693e-01,  2.4702e-01,  8.3709e-02,  6.6162e-01,\n",
       "          3.4056e-01,  7.3460e-02,  1.3157e-01, -1.4723e-01, -4.4024e-02,\n",
       "          3.8547e-01,  1.8999e-01, -3.0044e-01, -6.2470e-01,  6.7924e-01,\n",
       "          6.6659e-02, -1.8976e-01, -8.6098e-02, -1.4684e-01,  2.6377e-01,\n",
       "         -4.0001e-01, -3.1877e-01, -1.1865e-01, -9.0063e-02, -7.5715e-02,\n",
       "         -2.0228e-01,  5.4922e-01,  3.4077e-02, -9.2270e-01, -1.0850e-01,\n",
       "         -3.6692e-01,  2.6570e-01, -1.8803e-01, -2.2321e-01, -1.7080e-02,\n",
       "         -1.9230e-01,  7.7119e-02, -1.9381e-01,  1.9629e-01, -3.7294e-01,\n",
       "          4.1340e-01, -2.0172e-01,  5.0469e-01,  2.9860e-01,  1.2682e-01,\n",
       "          3.8624e-01, -1.0307e-01,  7.8352e-01, -3.2593e-01,  5.7028e-01,\n",
       "         -6.4098e-01, -3.3071e-01,  7.1062e-02, -8.8505e-01, -1.3585e-01,\n",
       "          4.8163e-01, -3.0467e-01, -1.7048e-01,  3.5114e-01, -6.7108e-02,\n",
       "         -3.1958e-01, -4.0711e-02,  1.8189e-02,  1.0407e+00, -1.6014e-01,\n",
       "          1.6126e+00,  3.3767e-01,  3.4924e-01,  5.2308e-01, -1.7524e-01,\n",
       "          2.8208e-02, -6.5593e-02,  2.5065e-01,  4.1909e-01,  4.0710e-01,\n",
       "          3.2762e-01, -3.8379e-01, -6.5249e-02,  2.4264e-01, -2.6448e-01,\n",
       "         -2.1804e-02, -6.6460e-01, -3.9100e-02, -1.3154e-01,  7.0088e-01,\n",
       "          4.1120e-01,  3.7272e-01,  1.1952e-01, -1.7101e-02,  3.9854e-01,\n",
       "         -1.1089e+00,  2.2088e-02, -1.4732e-01, -1.5172e-01,  2.9988e-01,\n",
       "          9.8365e-02,  5.8255e-02, -3.5410e-02, -6.7819e-02,  4.4599e-01,\n",
       "         -1.0485e+00,  2.7191e-01, -3.1847e-01, -5.4226e-02, -2.8260e-01,\n",
       "         -5.4552e-01, -4.8451e-01, -4.5215e-01,  9.9809e-02,  1.9465e-01,\n",
       "          1.8857e-01, -6.0594e-01, -3.9275e-02, -2.9343e-01, -2.7306e-01,\n",
       "          2.3421e-01,  6.4721e-02,  7.4255e-01, -9.7996e-02, -7.2048e-02,\n",
       "         -1.2939e-01,  1.0531e-01, -2.4164e-01,  4.1876e-01,  8.3530e-01,\n",
       "         -2.1851e-01, -1.7725e+00, -1.0203e-01,  2.5357e-03,  4.6857e-02,\n",
       "          2.0681e-02,  2.6083e-01, -1.3869e-01, -3.9625e-01, -9.8513e-02,\n",
       "         -1.5147e-01, -5.2543e-01, -5.9239e-01,  3.9604e-01,  1.0674e+00,\n",
       "          9.5916e-02,  5.7667e-01, -1.8765e-01,  4.6582e-01,  1.4298e-01,\n",
       "          4.7308e-01,  2.7341e-01,  8.6600e-02, -4.1995e-01, -1.7285e-01,\n",
       "         -7.7342e-01, -3.4478e-01,  1.8827e-01, -6.0748e-01, -7.8749e-01,\n",
       "          1.4915e-01, -4.9949e-02,  1.3732e-01, -5.5620e-02, -8.5470e-01,\n",
       "         -1.2832e-01, -1.4392e-01, -3.3826e+00, -9.3551e-02, -1.4685e-01,\n",
       "         -2.4479e-01,  8.7101e-01, -1.1160e-01,  1.8545e-02, -4.8950e-03,\n",
       "         -2.0189e-01, -2.8750e-01, -6.9094e-01,  2.8249e-01,  6.6716e-01,\n",
       "         -6.1584e-01, -1.5102e+00,  3.5374e-01,  6.7008e-01, -1.1681e-01,\n",
       "          2.9906e+00,  1.8439e-01, -1.0379e-01,  4.8683e-01,  3.4287e-01,\n",
       "         -2.0115e-01,  1.2116e+00,  3.6667e-01, -7.5620e-01, -1.8262e-01,\n",
       "         -2.9802e-01,  3.6742e-01,  3.6849e-01, -2.7711e-01,  3.0007e-01,\n",
       "         -6.3448e-01, -4.7982e-01, -2.9730e-01, -1.5198e-01,  4.1296e-01,\n",
       "         -1.0202e-01,  3.3580e-01, -2.3091e-01,  5.6091e-01,  1.9289e-01,\n",
       "          1.2039e-01, -2.1307e-01,  4.3758e-02,  2.4622e-01,  8.1681e-02,\n",
       "         -3.8213e-02,  3.1147e-01, -1.7622e-01,  4.9486e-01,  1.8022e+00,\n",
       "          9.4881e-01,  4.0913e-02, -2.1766e-01,  4.4872e-01, -5.9241e-02,\n",
       "         -2.1799e-01, -8.3773e-02, -1.2998e-01, -7.3927e-01,  1.5319e-01,\n",
       "          6.2496e-01, -3.1790e-01, -5.9490e-01,  3.0851e-01,  2.7814e-01,\n",
       "          5.0233e-01, -3.2866e-01,  1.2205e-01, -9.5478e-02,  5.9901e-01,\n",
       "          2.1332e-01, -3.6693e-01, -3.1015e-01,  2.7163e-01,  2.9649e-01,\n",
       "          1.3496e-01, -2.6284e-01, -9.7070e-01,  2.0982e-02,  2.2186e-01,\n",
       "         -1.4409e+00,  1.4506e-01, -7.1409e-02, -1.7585e-01, -3.2884e-01,\n",
       "         -5.3885e-01,  8.9775e-02,  3.0973e-02, -3.3387e-02,  4.5568e-01,\n",
       "          5.5198e-01, -5.0117e-02,  3.3930e-01,  1.7308e+00, -8.0666e-01,\n",
       "         -3.0997e-01, -1.0867e-02,  1.5949e-01,  4.8879e-01, -5.5864e-02,\n",
       "          1.2409e-01, -4.2961e-02, -1.8059e-01, -1.2063e-01, -1.7773e-01,\n",
       "         -5.4739e-01, -2.4197e-04,  2.1347e-01,  1.1974e-01,  2.3692e-01,\n",
       "         -1.4354e-01,  1.9054e-02, -3.8855e-01,  8.0442e-02,  1.2780e-01,\n",
       "         -1.9862e-01, -1.5263e-01, -1.2620e-01,  1.5728e-01, -4.3491e-01,\n",
       "          3.0333e-01, -2.0954e-01, -6.0583e-02, -1.7426e-01,  8.3577e+00,\n",
       "         -3.5079e-01,  1.2666e-01, -1.8872e-01, -2.1504e-01,  2.3632e-01,\n",
       "         -1.5468e-01,  2.6229e-01, -1.5589e-01]], grad_fn=<SelectBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (1, 3, 224, 224)  # Default SigLIP input size\n",
    "dummy_input = torch.randn(input_shape)\n",
    "output = vision_model(dummy_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c60a4c0b-fc76-493a-88a9-ec2a9ea1680e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6195,  0.2554, -1.0134,  ..., -0.5341,  0.4381, -1.6009],\n",
       "         [ 0.4081,  0.3242, -0.4967,  ..., -1.4876, -1.8570, -1.9226],\n",
       "         [-0.7247,  1.4924, -1.0048,  ..., -2.0564,  0.2459,  0.4242],\n",
       "         ...,\n",
       "         [ 0.7666, -0.1956, -1.1861,  ..., -0.5558,  1.5912, -2.1294],\n",
       "         [ 0.0407,  0.3529, -1.6780,  ...,  0.6625, -0.1729, -1.9680],\n",
       "         [ 1.0815, -1.5693, -0.5918,  ..., -0.2207,  1.1262, -0.3552]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f038e65d-ad47-47cd-93c4-439a575f5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigLIPImageEncoder(torch.nn.Module):\n",
    "    \"\"\"Wrapper class for SigLIP's image encoder\"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.vision_model = model.vision_model\n",
    "        \n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vision_model(pixel_values)\n",
    "        # Get image embeddings - using mean pooling like in original model\n",
    "        image_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cb0b5e0-c93f-46a3-88a9-0087c05cf666",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = SigLIPImageEncoder(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61045678-7d55-4fac-ac4f-02594041687a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SigLIPImageEncoder(\n",
       "  (vision_model): SiglipVisionTransformer(\n",
       "    (embeddings): SiglipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "      (position_embedding): Embedding(196, 768)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): SiglipMultiheadAttentionPoolingHead(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): SiglipMLP(\n",
       "        (activation_fn): PytorchGELUTanh()\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11f3ce10-3549-4cc1-9298-cb10cf9112ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "onn_save_path = \"siglip_base_image_encoder.onnx\"\n",
    "torch.onnx.export(\n",
    "    image_encoder,\n",
    "    dummy_input,\n",
    "    onn_save_path,\n",
    "    export_params=True,\n",
    "    opset_version=15,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['pixel_values'],\n",
    "    output_names=['image_embeddings'],\n",
    "    dynamic_axes={\n",
    "        'pixel_values': {0: 'batch_size'},\n",
    "        'image_embeddings': {0: 'batch_size'}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedea21e-72dd-4e71-8fe3-272b4e744894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
